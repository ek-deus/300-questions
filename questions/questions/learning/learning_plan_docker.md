# Программа обучения Docker

## Содержание:

1.	Принцип работы, основные компоненты, установка

    [1.1.	Что такое Docker](#Docker)

    [1.2.	Что такое контейнер](#Container)

    [1.3.	Архитектура и компоненты Docker](#Component)

    [1.4.	Как работают контейнеры](#working)

    [1.5.	Что такое образы и их строение](#iso)

    [1.6.	Что такое репозиторий](#repo)

    [1.7.	Установка Docker](#Docker-install)

2.	Dockerfile

    [2.1.	Основы DockerFile](#DockerFile)

    [2.2.	Работа с инструкциями Dockerfile](#)

    [2.3.	Переменные окружения](#environment)

    [2.4.	Аргументы](#)

    [2.5.	Не привилегированный пользователь](#)

    [2.6.	Порядок выполнения инструкций](#)

    [2.7.	Инструкция Volume](#)

    [2.8.	Entrypoint vs Cmd](#)

    [2.9.	Dockerignore](#)    

3.	Работа с образами в Dockerfile

    [3.1.	Как создавать собственные образы, способы создания образа](#)

    [3.2.	Приложение Python c Dockerfile](#)

    [3.3.	Создание образа для приложения Python](#)

    [3.4.	Запуск контейнера с приложением Python](#)

    [3.5.	Создание другой версии образа](#)

    [3.6.	Анализ файловой структуры контейнера Python](#)

    [3.7.	Многоэтапная сборка](#)

    [3.8.	Тэги](#)

    [3.9.	История образов](#)

    [3.10.	Сохранение и загрузка образов](#)

    [3.11.	Работа с Docker registry](#)

    [3.11.1. Local](#)
    
    [3.11.2. Remote](#)    

4.	Основные команды и создание контейнеров

    [4.1.	Основные команды Docker](#command)

    [4.2.	Доступ в интернет внутри контейнеров](#)

    [4.3.	Создание контейнера NGINX](#)

    [4.4.	Запуск контейнера в фоновом режиме](#)

    [4.5.	Остановка контейнеров](#)

    [4.6.	Запуск дополнительных процессов в работающем контейнере](#)

    [4.7.	Создание имени для контейнера](#)

    [4.8.	Выполнение команд в контейнере](#)

    [4.9.	Логи контейнеров](#)

    [4.10.	Публикация портов контейнера](#)

    [4.11.	Разные порты для разных контейнеров](#)

    [4.12.	Автоматическое удаление остановленных контейнеров](#)

    [4.13.	Разделение команды на строки](#)

5.	Управление контейнерами

    [5.1.	Просмотр статистики контейнеров](#)

    [5.2.	Автоматический запуск контейнеров](#)

    [5.3.	Просмотр событий в Docker](#)

    [5.4.	Управление остановленными контейнерами](#)

    [5.5.	Portainer](#)

    [5.6.	Watchtower](#)


6.	Сеть

    [6.1.	Основы сетей Docker](#)

    [6.2.  Разновидности сетей Docker](#)

    [6.3.	Основные команды сетей Docker](#)

    [6.4.	Работа с сетями контейнера](#)

7.	Хранение данных

    [7.1.	Основы хранения данных в Docker](#)

    [7.2.	Основные команды разделов](#)

    [7.3.	Bind Mount](#)

    [7.4.	Работа с разделами](#)


8.	Docker-compose

    [8.1.	Что такое docker-compose](#)

    [8.2.	Состав docker-compose](#)

    [8.3.	Основные команды Docker-compose](#)

    [8.4.	Файл конфигурации Docker-compose](#)

    [8.5.	Разделы и сети в compose](#)

    [8.6.	Установка и управление несколькими контейнерами на выделенном узле](#)

9.	Docker Swarm

    [9.1.	Основы Docker Swarm](#)

    [9.2.	Запуск Docker в режиме Swarm](#)

    [9.3.	Управление узлами Docker](#)

    [9.4.	Работа с сервисами в Docker Swarm](#)

    [9.5.	Работа с сетями в Docker Swarm](#)

    [9.6.	Работа с разделами в Docker Swarm](#)

    [9.7.	Работа со стаками в Docker Swarm](#)

10.	Безопасность Docker

    [10.1.	Основы безопасности Docker](#)

    [10.2.	Основы работы с безопасностью](#)

    [10.3.	Система Docker Content Trust](#)

    [10.4.	Секреты](#)

11.	Вопросы производительности

Пример запуска фронтэнд и бэкэнд приложения + бд

---

# 1. Принцип работы, основные компоненты, установка

## <a name="Docker"> </a> 1.1. Что такое Docker, его преимущества

**Docker** - это сервис (программная платформа) для запуска приложений в контейнерах, с помощью которого можно автоматизировать создание приложений, их доставку и управление.

Преимущества использования Docker:

- Возможность отката. В любой момент контейнер можно «сбросить», откатить до изначальной версии — со всем сервером так сделать не получится. Это позволяет эффективнее тестировать приложения и легче исправлять ошибки. Плюс контейнер всегда можно перезагрузить, как обычный компьютер, — иногда это тоже помогает исправить ошибку.
- Готовая среда для запуска. Контейнер вместе с приложением сразу содержит среду для работы. В этой среде мы пишем приложение и в ней же его тестируем, чтобы убедиться в работоспособности. Это позволяет загрузить готовый контейнер на любой сервер — и быть уверенным, что приложение запустится нормально. Неважно, где и как код писали, запускаться он будет стабильно именно благодаря упаковке в контейнер.
- Небольшой вес. Виртуальные машины могут весить несколько гигабайтов. Docker-контейнер чаще всего весит не больше пары сотен мегабайтов, иногда сильно меньше. Он быстро запускается и не требует больших вычислительных мощностей.
- Изолированность от внешней среды. У docker-контейнера нет доступа к информации на хосте. Это делает работу приложения более безопасной.
- Удобные инструменты управления. Образы docker хранятся в удобном docker-реестре, откуда можно в пару кликов запускать готовые среды и на их основе настраивать свои. Сам Docker позволяет управлять контейнерами: запускать, сохранять, редактировать, перезагружать. А если контейнеров в работе много, существуют специальные инструменты для массового управления — оркестрации. Для docker-контейнеров стандартным инструментом считается Kubernetes.

## <a name="Container"> 1.2. Что такое контейнер

**Контейнер** — это среда, внутри которой имитируется определённая операционная система. В эту систему мы можем положить код и запускать его в конкретной изолированной среде и в определённых нами условиях. 

Как правило, в одном контейнере запускают одно приложение или даже отдельный его компонент — модуль, функцию или микросервис.
Контейнер чем-то похож на виртуальную машину, только он гораздо компактнее и проще устроен. Он не требует выделять ему конкретные ресурсы, как виртуальная машина, а работает прямо на ресурсах нашей операционной системы.



**!!!Рисунок контейнер!!!**



## <a name="Component"> 1.3. Архитектура и компоненты Docker

Базовая часть Docker состоит из четырёх основных компонентов: клиента, демона, хоста и реестра.
Клиент запускается в командной строке и с помощью него мы подключаемся к службе (демону) Докер. А служба запущена каком-то хосте.
 


**!!!Рисунок компоненты!!!**



Рассмотрим все компоненты Docker:

- **Daemon**. Фоновая служба на хосте, которая отвечает за создание, запуск и уничтожение контейнеров. /  центральный системный компонент, который управляет всеми процессами докера: создание образов, запуск и остановка контейнеров, скачивание образов. Работает Docker daemon как фоновый процесс (демон) и постоянно следит за состоянием других компонентов.
- **Client**. Утилита командной строки в Docker для управления демоном. Любое взаимодействие с контейнером проходит через Daemon. / Это утилита, предоставляющая API к докер-демону. Клиент может быть консольным (*nix-системы) или графическим (Windows).
- Host. Компьютер, на котором запущен Docker. / Это просто компьютер или виртуальный сервер, на котором установлен Docker. Кстати, Docker можно запустить и из WSL 2.
- **Container**. Запущенное приложение, которое развернули из образа.
- **Image**. Неизменяемый файл (образ), из которого можно неограниченное количество раз развернуть контейнер. / Это шаблон (физически — исполняемый пакет), из которого создаются Docker-контейнеры. Образ хранит в себе всё необходимое для запуска приложения, помещенного в контейнер: код, среду выполнения, библиотеки, переменные окружения и конфигурационные файлы.

**Docker-образ** создаётся с помощью команды docker build, которая считывает конфигурацию создаваемого образа из специального конфигурационного файла — **dockerfile**.

В **Dockerfile** записываются команды и опции создания образа, а также некоторые настройки будущего контейнера, такие как порты, переменные окружения и другие опции.

Каждая команда записанная в **dockerfile** создаёт свой **слой**. Чем больше слоёв, тем дольше будет собираться образ и дольше загружаться контейнер. Финальный Docker-образ — это объединение всех слоев в один. Благодаря такому подходу можно переиспользовать уже готовые образа для создания новых образов.
- **Repository**. В репозитории находятся различные версии образов.
- **Registry**. В реестре хранятся различные репозитории. Реестр может быть локальным или удаленным.
- **Docker volumes**. Тома для постоянного хранения информации. По умолчанию в Docker папки хранилищ создаются на хост-машине, но предусмотрена и возможность подключения удаленных хранилищ. Использование томов позволяет лучшим образом настроить хранение данных.
- Dockerfile. Текстовый файл с последовательно расположенными инструкциями для создания образа Docker. Если Docker image — это пирог, то **Dockerfile** — рецепт его приготовления. В этом файле описаны основные инструкции для сборки образа: какой базовый образ взять, откуда и куда положить файлы и так далее.
- **Docker Desktop**. Приложение, позволяющее локально собирать, выполнять и тестировать контейнеры. Работает на **Windows** и **macOS**.

## <a name="working"></a> 1.4. Как работают контейнеры

Есть Хост на Linux, с ресурсами ядра Linux, CPU, RAM, Network и дисковым пространством.

Также на нем есть Docker Engine, который запускает Docker daemon.

При создании контейнера создадутся изолированные файлы на жестком диске, которые доступны только этому контейнеру.

Далее для этого контейнера создадутся изолированные процессы, которые используют общее ядро Linux.

Важно понять, что контейнер работает на ядре хостовой ОС и изолируется средствами операционной системы, а не возможностями железа, как виртуальная машина.

Таким образом можно запустить еще контейнеры, у которых будут свои файлы на жестком диске, свои выделенные ресурсы CPU и RAM, и процессы внутри контейнеров также будут независимы.

**Важно!** Если вы создаете несколько контейнеров с одного и того же образа, то у этих контейнеров будут определенные общие файлы. Эти общие файлы будут находиться на жестком диске Докер хоста только в одном месте. То есть они не будут скопированы несколько раз в зависимости от количества контейнеров какого-то образа.

После остановки и удаления контейнеров все соответствующие файлы, которые созданы этими контейнерами, удаляются.
Докер автоматически останавливает те контейнеры, у которых нет активных процессов. То есть, если какой-то процесс внутри контейнера выполнил свою задачу и завершился, то Докер автоматически остановит такой контейнер.

**Подведем итог:** Все контейнеры на одном Докер хосте распределяют между собой все доступные ресурсы, а также используют общее ядро Linux.



**!!!Рисунок контейнеры!!!**
 


Детальное описание контейнеризации:

По сути контейнер — это определенные пространства имён (Namespaces) и наборы контрольных групп (Control groups), удобно управляемые с помощью сторонних утилит. Например, с помощью Docker.

С помощью Namespaces (неймспейсы) процессы можно объединить в группы и изолировать, а с помощью Cgroups можно задать лимиты по ресурсам.

```
Namespace — механизм изоляции процессов. Мы можем создавать пространства имён процессов (группы процессов) помещать туда нужные нам процессы по их идентификаторам — пидам (PID) и эти процессы не могут обращаться к процессам вне своего пространства имён.
```

Свежим глотком воздуха стало появление в 2002 году, в ядре Linux версии 2.4.19 нового API для создания абстракции контроля над общими ресурсами — **Namespaces API**.

Namespaces (пространства имен) — это абстракция (программная прослойка) над физическими ресурсами. Если раньше процессы обращались напрямую к ресурсам, то с появлением namespaces, все запросы проходят через этот дополнительный слой абстракции.

С появлением namespace в ядро было добавлено 3 новых функции, которые отвечают за управление атрибутами namespace:
- **clone**() — аналог **fork**() с возможностью выделения частей общих ресурсов в отдельные namespaces.
- **setns**() — подключает указанный процесс к заданному namespace.
- **unshare**() — изменение контекста текущего процесса.

В результате вызовов перечисленных функций ядра, namespaces становятся новыми атрибутами процессов. Это позволяет разным процессам иметь разное представление о тех глобальных ресурсах, которыми они распоряжаются.

Существуют следующие пространства имён:

**Mount** — абстракция над пространством имен для файловых систем. Mount позволяет сразу монтировать новое устройство в несколько пространств имён файловой системы, вместо монтирования в каждом отдельном пространстве.

**Network** — абстракция над сетью: интерфейсы, таблицы маршрутизации и т.д. Пространство имен Network по сути выполняет роль туннеля между разными пространствами имён сети.

**IPC** — абстракция над межпроцессным взаимодействием. Процесс в пространстве имен IPC не может писать или читать IPC ресурсы, принадлежащие другому пространству имен. Так процессы в одном контейнере не могут вмешиваться в другие контейнеры.

**PID** — абстракция над пространством номеров процессов. **PID** изолирует пространство ID процессов. Процессы в различных пространствах могут иметь одинаковые ID.

**User** — абстракция над пространством пользователей. User изолирует **ID** пользователей и **групп**, корневой каталог, ключи и **capabilities**.

**UTS (Unix Time Sharing)** — абстракция над пространством hostname и NIS. UTS позволяет контейнерам иметь собственные доменные имена NIS domainname и имена контейнеров nodename.

**Cgroup** — используется как атрибут, корневой узел дерева cgroup.

Перечисленные неймспейсы появлялись постепенно, по-мере необходимости решения определенных технических задач. 

Неймспейсы решили вопросы изоляции, однако вопрос ограничения ресурсов для изолированных процессов оставался открытым. Решение этого вопроса появилось с выходом ядра версии 2.6.20 в 2008 году — в нём появился механизм Cgroups.

```
Control groups — механизм определения количества выдаваемых ресурсов процессам.
```

**Cgroups (control group)** — группа процессов Linux, на которые наложена изоляция и установлены ограничения на вычислительные ресурсы (процессорные, сетевые, ресурсы памяти, ресурсы ввода-вывода) со стороны ядра Linux.

Cgroups состоит из двух частей: cgroup core (ядро cgroup) и подсистем (директории с управляющими файлами).
Количество подсистем может отличаться в зависимости от версии ядра. Так в ядре версии 4.4.0.21 таких подсистем 12, в ядре версии 5.10.16.3, которое содержится в Ubuntu 20 TLS, подсистем уже 13:

1.	**blkio** — управляет лимитами чтения и записи на блочных устройствах;
2.	**cpu** — управляет доступом к ресурсам процессора;
3.	**cpuset** — выделяет отдельных процессоров группе;
4.	**cpuacct** — управляет ресурсами cpu (работает совместно с **cpu**);
5.	**devices** — ограничивает доступ к устройствам;
6.	**hugetlb** — управляет работой групп с большими страницами памяти (huge pages);
7.	**net_cls** — размечает спец. тегами сетевые пакеты, что позволяет процессам генерирующим их определять какие процессы их сгенерировали;
8. **perf_event rdma** — предоставляет интерфейс для инструмента анализа производительности в Linux (perf);
9.	**freezer** — управляет приостановкой и возобновлением процессов выполнения задач внутри контрольной группы;
10.	**memory** — управляет выделением памяти для групп процессов;
11.	**net_prio** — управляет динамическим назначением приоритета трафика;
12.	**pids** — ограничивает количество процессов в рамках контрольной группы;
13.	**unified** — автоматически монтирует файловую систему в каталог **/sys/fs/cgroup/unified** при запуске системы.

Вывести список подсистем в вашей версии ядра можно следующей командой:

```
$ ls /sys/fs/cgroup/
blkio cpuacct devices hugetlb net_cls perf_event rdma cpu cpuset freezer memory net_prio pids unified
```

Посмотреть к каким контрольным группам принадлежит процесс можно так:
- вызовите утилиту htop
- найдите нужный вам PID процесса
- перейдите в каталог /proc и найдите там директорию с нужным вам PID
- вызовите команду cat и передайте ей cgroup

```
$ cd /proc/PID/
$ cat cgroup
39:rdma:/
38:pids:/
37:hugetlb:/
36:net_prio:/
35:perf_event:/
34:net_cls:/
33:freezer:/
32:devices:/
31:memory:/
30:blkio:/
29:cpuacct:/
28:cpu:/
27:cpuset:/
0::/
```

Как работать с Cgroups

Работать с cgroups можно двумя способами:
1. Стандартным способ — работать с cgroups как с файлами. Вносить записи групп с помощью echo в нужный контроллер.
2. С помощью cgroup-tools — работать с cgroups через набор утилит, которые облегчают взаимодействие с cgroups.

Первый способ более предпочтительный, но более трудозатратный для пользователей, имеющих не богатый опыт администрирования Linux, поэтому мы пойдем вторым путем — будем использовать cgroup-tools.

Поместить процесс в контрольную группу просто. Достаточно выполнить команду cgcreate, передать ей следующие параметры:

•	**-t uid:gid** — пользователи, получающие права на перемещение заданий в (из) группы.
•	**-a uid:gid** — пользователи, получающие права на управление параметрами группы (опциональный параметр).
•	**-g** список подсистем:путь.

Пример создания контрольной группы по памяти и процессору:
```
sudo cgcreate -t root:root -g memory,cpu:/mycgroup
```
Теперь в нашу вновь созданную группу нужно переместить процессы. Для этого есть команда cgclassify, в которую с помощью ключа g передается группа, её название и пиды процессов:
```
sudo cgclassify -g cpu:/mycgroup 7
```
Убедится, что процесс попал в новую группу можно с помощью команды cat:
```
cat /proc/PID-номер/cgroup
```
Удалить процесс из группы можно также одной командой:
```
sudo cgdelete -g memory,cpu:/mycgroup
```
До этого мы умели лишь создавать группы и помещать в них процессы. Теперь научимся задавать ограничение ресурсов. Делается это командой cgset с параметром -r:
```
cgset -r cpu.shares=1 /mycgroup
```
Мы рассмотрели несколько основных инструментов управления контрольными группами из пакета cgroups-tools. Их там конечно же куда больше. Полное описание всех утилит пакета можно посмотреть тут.  https://manpages.debian.org/testing/cgroup-tools/index.html
Кстати, есть и альтернативный способ управления и настройки cgroups, о нём можно узнать из этой статьи: https://linux-notes.org/ustanovka-cgroups-v-unix-linux/

## <a name="iso"></a> 1.5. Что такое образы и их строение

Образ это набор файлов, он статичен. С этого статичного образа можно создавать контейнеры.  При этом с одного образа можно создать несколько контейнеров. Контейнер отличается от образа тем, что в контейнере есть запущенные процессы. Информация для запуска того, что будет внутри контейнера, как раз находится в образе. То есть контейнеры разворачиваются из образов. / Образ — это только шаблон для создания контейнеров. В основе любого образа лежит родительский образ, который, как правило, содержит ОС. Образ состоит из слоев. Каждая команда в Dockerfile создаёт новый слой. Образы можно переиспользовать много раз. Образы можно загружать в удаленный репозиторий.

Строение образа:



## <a name="repo"></a> 1.6. Что такое репозиторий

Репозиторием Docker (Docker Repository) называют набор образов Docker, обладающих одинаковыми именами и разными тегами. Теги — это идентификаторы образов.

Обычно в репозиториях хранятся разные версии одних и тех же образов. Например, Python — это имя популярнейшего официального репозитория Docker на хабе Docker. А вот Python:3.7-slim — это версия образа с тегом 3.7-slim в репозитории Python. В реестр можно отправить как целый репозиторий, так и отдельный образ.

## <a name="Docker-install"></a> 1.7. Установка Docker

Для работы на Linux сразу устанавливается Docker engine

Для работы на Mac или Windows при помощи программы Docker Desktop создается виртуальная машина, внутри которой запустится Докер демон, и все контейнеры будут запущены внутри этой виртуальной машины.


Docker устанавливается в несколько простых шагов:

Сначала обновим существующий список пакетов
```
sudo apt update
```
Установим несколько маленьких утилит, которые позволяют apt использовать пакеты через HTTPS
```
sudo apt install apt-transport-https ca-certificates curl software-properties-common
```
Добавим GPG-ключ для официального репозитория Docker
```
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
```
Добавим репозиторий Docker в источники APT
```
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable"
```
На этом этапе может возникнуть некритическая ошибка, связанная с недоступностью одного из репозиториев Docker: `404 Not Found [IP: 52.85.47.84 443»`. Не пугайтесь, это не отразится на целостности установки Docker.

Обновим базу данных пакетов еще раз
```
sudo apt update
```
Убедимся, что установка будет выполняться из репозитория Docker, а не из репозитория Ubuntu
```
apt-cache policy docker-ce
```
Команда должна вернуть вывод следующего вида
```
docker-ce: Installed: (none) Candidate: 5:20.10.12~3-0~ubuntu-focal Version table: 5:20.10.12~3-0~ubuntu-focal 500 500 https://download.docker.com/linux/ubuntu focal/stable amd64 Packages
```
Обратите внимание, что docker-ce не установлен, но является кандидатом для установки.

```
sudo apt install docker-ce
```
Проверим работоспособность демона Docker
```
sudo systemctl status docker
```

# 2.	Dockerfile

## <a name="DockerFile"></a> 2.1.	Основы DockerFile

Dockerfile — это конфигурационный файл, в котором описаны инструкции, которые будут применены при сборке Docker-образа и запуске контейнера. Dockerfile создается в корневой директории проекта и не имеет расширения.

Синтаксис Dockerfile близок к синтаксису конфигурационных файлов .ini. На каждую строчку приходится одна инструкция. Инструкции пишутся капсом, а их значения отделяются пробелом.

Каждая новая инструкция — новый слой. В качестве инструкции можно выполнить команду в терминале, скопировать файлы внутрь образа или настроить связь с внешним миром с помощью сетевого окружения и томов. Docker объединяет файловые системы отдельных слоёв в одну во время сборки, используя механизм Union File Systems.

Концепция слоёв позволяет Docker оптимальным образом хранить данные на жёстком диске. Docker загружает только те слои, которых не было на компьютере прежде. При этом слой из одного образа может подойти и к другому.

После того как слои образа описаны в файле конфигурации, необходимо произвести сборку образа с помощью команды:

```
docker build
```

Существуют определенные правила и логика заполнения **Dockerfile**. Разберем базовые инструкции на примере **Dockerfile** с иллюстрации выше.

1. **FROM**

Любой код или набор инструкций выполняется сверху вниз. Поэтому **Dockerfile** всегда начинается с открывающей инструкции FROM, которая говорит демону **Docker**, какой образ для основы нужно взять. Если образа локально нет — он будет скачан с **Docker hub**.

 
2. **RUN**

Далее идет инструкция **RUN** с определенной командой. В нашем случае мы использовали команду **mkdir -p** для создания папки **/ap**p. Инструкций **RUN** может быть неограниченное количество, но стоит помнить, что каждая инструкция создает свой слой, поэтому хорошей практикой является запись цепочки команд через &&: R**UN comand_1 && comand_2 && comand_3**.

 
3. **WORKDIR**

Есть инструкции логически связанные между собой. Инструкция WORKDIR устанавливает активный рабочий каталог. Все последующие команды, такие как **COPY**, **RUN**, **CMD** и некоторые другие будут выполнены из рабочего каталога, установленного через **WORKDIR**.

 
4. **COPY**

С инструкцией COPY всё просто. Первым аргументом указывается папка для копирования, а вторым аргументом — папка в контейнере куда будут помещены файлы из копируемой директории. В нашем случае, из текущего каталога (“.” — каталог, в котором находится пользователь, “..” — каталог выше относительно “.”.), в котором находится пользователь, был скопирован файл **requirements.txt** и помещен в папку **/app** в контейнере.

 
5. **RUN**

Инструкцию RUN мы рассмотрели ранее. Тут лишь хотим обратить ваше внимание на её поведение в сочетании с инструкцией WORKDIR. Ранее инструкция COPY перенесла файл requirements.txt в контейнер. Кстати, в качестве финального пути мы могли указать “.”, так как инструкция WORKDIR установила в качестве рабочей директории контейнера папку** /app**. И теперь команда RUN будет выполнена именно из директории **/app**.
 
6. **CMD**

Финальной инструкцией в любом **Dockerfile** является **CMD** или **ENTRYPOINT**. В отличие от других инструкций **CMD** может быть только одна и она может быть переопределена при старте контейнера командой **docker run**. Инструкция CMD наследует условия установленные инструкцией **WORKDIR**.

Не все инструкции указанные в **Dockerfile** непосредственно исполняются при сборке образа и запуске контейнера. Например, инструкция **EXPOSE** лишь говорит демону Docker, что мы намереваемся пробросить указанный нами порт наружу контейнера —** EXPOSE 80**. В этом примере мы хотим пробросить порт 80 изнутри контейнера наружу.

Однако после запуска контейнера «постучаться» на 80 порт к нему мы не сможем. Так как для подтверждения инструкции EXPOSE используется ключ -P команды docker run. Если требуется прокинуть порт и переназначить его снаружи — используется следующая команда: `docker run -p 80:80 --name test_cont -d`).



## <a name="environment"></a> 2.3.	Переменные окружения

Dockerfile имеет следующую логику заполнения:

Первой инструкцией всегда идёт FROM с указанием родительского образа. Например, FROM python:latest.
Инструкция RUN может принимать конвейер команд Linux, чтобы не создавать лишние слои. Например, RUN apt-get update && apt-get install python3-pip -y && pip install --upgrade pip && pip install pipenv.
Инструкция WORKDIR устанавливает рабочий каталог контейнера. Например, WORKDIR /usr/src/app/. Последующие команды RUN, CMD, ENTRYPOINT наследуют привязку WORKDIR.
Завершающей инструкцией всегда идёт CMD. Например, CMD ["python", "web_interface.py"]. CMD наследует привязку к WORKDIR, поэтому web_interface.py будет запущен из папки /usr/src/app/.

```
FROM python:latest
RUN apt-get update && apt-get install python3-pip -y && pip install --upgrade pip && pip install pipenv
RUN mkdir -p /usr/src/app/
WORKDIR /usr/src/app/
COPY . /usr/src/app/
EXPOSE 5000
RUN pip install --no-cache-dir -r requirements.txt
CMD ["python", "web_interface.py"]
```

Dockerfile, как и положено, открывается инструкцией FROM с указанием родительского образа. Выбрать подходящий образ можно на hub.docker.com или с помощью команды docker search имя образа. Например, docker search python.

Далее следуют несколько команд RUN. Поскольку каждая инструкция создает новый слой — хорошей практикой считается писать конвейеры логически связанных команд в одной инструкции RUN. Например, мы поместили команды обновления пакетов apt, установку пакетного менеджера pip и его обновление в одну инструкцию. Для комбинирования команд используется логический оператор «И», обозначающийся как &&.

После серии инструкций RUN идет инструкция WORKDIR, которая устанавливает рабочую директорию контейнера. Все последующие команды, работающие с привязками к директории, такие как RUN, CMD, ENTRYPOINT будут выполняться исходя из установленной директории.

Далее идёт инструкция EXPOSE, которая выражает намерение открыть заданный порт. Инструкция сама по себе не открывает порт без применения команды docker run с ключом -P. Если нужно «повесить» контейнер на определённый внешний порт с переадресацией во внутренний порт контейнера — применяется ключ -p с указанием внутреннего и внешнего порта через «:». Например, docker run -p 5000:8080.

Предпоследней идет опять инструкция RUN. Мы уже рассматривали её выше, однако здесь она исполняется с зависимостью от WORKDIR. Инструкция WORKDIR в качестве рабочей директории установила /usr/src/app/, поэтому текущая инструкция RUN будет выполнять команду pip install --no-cache-dir -r requirements.txt из директории установленной инструкцией WORKDIR.

Завершающая инструкция в нашем Dockerfile — CMD с командой ["python", "web_interface.py"]. Опять же заметим, что здесь CMD связана с WORKDIR и скрипт web_interface.py будет выполнен из директории /usr/src/app/. Важно запомнить, что CMD всегда идет последней и должна быть в единственном экземпляре.

Приведем таблицу инструкций Dockerfile с примерами команд и опций:

| Инструкция | Описание | Пример использования | Комментарий |
| ------ | ------ |------ |------ |
| **FROM**	 | Задает базовый образ. Все последующие инструкции создают слои поверх родительского образа. | _FROM python:latest FROM debian:wheezy_ | Быстрее всего можно найти образ с нужным тегом на Docker hub. |
| **RUN** | Выполняет команду внутри контейнера и сохраняет результат. |_RUN mkdir /usr/src/app/ RUN apt-get update && apt-get install python3-pip -y_| RUN может исполнять конвейер команд с логическими операторами && и ||. |
| **COPY** | Копирует файлы и папки из текущей директории, где находится пользователь в указанную директорию в контейнере |_COPY . /usr/src/app/_| COPY считывает позицию пользователя на хосте, поэтому первым аргументом идет «.» |
| **ADD** | Копирует файлы и папки из текущей позиции пользователя, скачивает файлы по URL и работает с tar-архивами |_ ADD https://ekdeus.me/archive/api_config.ini/usr/src/app/_ | Официальная документация не рекомендует применять ADD. Для скачивания по URL можно использовать RUN с CURL или WGET, а для копирования — COPY. |
| **CMD** | ceВыполняет команду с указанными аргументами во время запуска контейнера.ll | _CMD ["python", "web_interface.py"]_ | CMD должна быть одна в конце Dockerfile. CMD может вызывать исполняемый файл — .sh Аргументы docker run переопределяют CMD. Если в Dockerfile нет CMD, обязательно должна быть инструкция ENTRYPOINT. |
| **ENTRYPOINT** | Похожа на CMD, но при запуске контейнера не переопределяется в отличие от CMD. | _ENTRYPOINT ["python", "web_interface.py"]_ | ENTRYPOINT может использоваться совместно с CMD. |
| **ENV** | Задает переменные среды внутри образа, на которые могут ссылаться другие инструкции |_ENV ADMIN="ek"_ | ENV часто применяется для передачи информации в контейнеризированное приложение через переменные среды. |
| **ARG** | Задает переменные, значение которых передается докером во время сборки образа |_ARG maintainer=ek_ | В отличие от ENV-переменных, ARG-переменные недоступны во время выполнения контейнера. |
| **WORKDIR** | Устанавливает рабочую директорию контейнер | _WORKDIR /usr/src/app/_ | Последующие инструкции CMD, RUN, ENTRYPOINT наследуют привязку к директории установленной WORKDIR. |
| **VOLUME** | Создает и подключает постоянный том хранения данных |_VOLUME /data_cont_1_ | Просмотреть существующие тома можно командой docker volume ls. К контейнеру можно подключить существующий том, для этого достаточно указать уже существующий том. |
| **EXPOSE** | Указывает планируемый рабочий порт у контейнера. Инструкция сама по себе не открывает порт. Чтобы использовался указанный в EXPOSE порт — нужно указать docker run -P при запуске контейнера. |_EXPOSE 5000_| Если требуется пробросить и сопоставить разные порты внутри и снаружи контейнера используется docker run -p внутренний порт:внешний порт |
| **LABEL** | Добавляет метаданные в образ. |_LABEL maintainer="Evgenii_Kiselev_" | Обычно LABEL содержит информацию об авторе образа. |
| ------ | ------ |------ |------ |



### Описание инструкций Dockerfile  

**FROM. Установка базового образа** 

Dockerfile обычно начинается с инструкции FROM. Эта инструкция задаёт базовый образ.

В качестве базового образа может быть использован образ с чистой операционной системой, образ с уже установленной и настроенной платформой или вообще любой другой образ. Вот так можно установить Ubuntu 18.04 как базовый образ:

```
FROM ubuntu:18.04
```

Для веб-приложения на Node.js обычно используют официальный образ от команды Docker:

```
FROM node
```

**RUN. Запуск команд терминала** 

Инструкция RUN позволяет запускать команды терминала при сборке. Это самая используемая инструкция, ей можно создать папку, установить недостающие пакеты или запустить shell скрипт.

Например, установим платформу Node.js поверх образа с чистой Ubuntu:

```
FROM ubuntu:18.04
RUN sudo apt update && sudo apt install nodejs && sudo apt install npm
```

При сборке образа теперь будет произведена установка последней версии Node.js.

**COPY и ADD. Копирование файлов проекта**

Инструкции COPY и ADD позволяют перенести файлы с компьютера, который запускает сборку, внутрь образа.

Например, перенесём все содержимое папки, где лежит Dockerfile в папку /app внутри образа:

```
FROM ubuntu:18.04
RUN sudo apt update && sudo apt install nodejs && sudo apt install npm
COPY . /app
```

Используйте COPY, она только копирует указанную папку во внутреннюю папку образа. Инструкция ADD слишком всемогущая и можно случайно использовать её неверно. Например, она может скачать файл из Интернета перед копированием или разархивировать архив.

**ENTRYPOINT и CMD. Запуск приложения** 

После того как образ готов, необходимо запустить приложение, которое в нем содержится. Образы Docker задумывались как упаковка для приложения, поэтому нет ничего удивительного в существовании механизма запуска приложения при старте контейнера на основе собранного образа. Для этого используют одну из двух инструкций: ENTRYPOINT и CMD.

Инструкция ENTRYPOINT используется для запуска приложения при старте контейнера:

```
FROM ubuntu:18.04
RUN sudo apt update && sudo apt install nodejs && sudo apt install npm
COPY . /app
ENTRYPOINT ["node", "/app/app.js"]
```

В отличие от инструкции RUN эта инструкция получает полный доступ к инфраструктуре терминала на компьютере пользователя. Вместе с командой запуска контейнера вы можете передавать параметры команде, которая прописана после ENTRYPOINT или пользоваться системой сигналов Linux. Внутрь образа можно положить программу и запускать её внутри контейнера, передавая через параметры текстовые файлы со своего компьютера. Например, можно упаковать в контейнер утилиту для проверки орфографии yaspeller. В примере ниже она используется для проверки орфографии слов на русском и английском языках в файлах с расширением *.md и *.txt:

```
FROM node
RUN npm install yaspeller -g
ENTRYPOINT ["yaspeller"]
```

Затем необходимо собрать образ, указав явно имя образа для удобства:

```
docker build -t yaspeller .
```

Запускать проверку орфографии в любой папке для файлов с расширением *.md и *.txt можно теперь простой командой:

```
docker run --rm yaspeller .
```

Вместо . можно писать название файла или папки. Ключ --rm означает, что после завершения работы контейнер удалится из списка использованных Docker. Это важно, поскольку, пока контейнер хранится в этом списке, нельзя запустить контейнер с таким же именем, несмотря на то, что контейнер уже отработал и не используется.

Инструкция CMD делает практически то же самое. Обычно это также команда запуска приложения:

```
FROM ubuntu:18.04
RUN sudo apt update && sudo apt install nodejs && sudo apt install npm
COPY . /app
CMD ["node", "/app/app.js"]
```

CMD — инструкция запуска по умолчанию, она игнорируется в том случае, если пользователь вашего образа прописывает в явном виде, что и как запускать после запуска контейнера на основе образа. Обычно CMD вообще используется для передачи параметров по умолчанию вашему приложению, которые пользователь может переопределить.

В чем же разница между ENTRYPOINT и CMD? В ваших намерениях.

Используйте ENTRYPOINT, если вы не хотите, чтобы пользователь вашего образа переопределял поведение приложения в контейнере. Используйте CMD, если записываете команду по умолчанию, которую пользователь с лёгкостью может переопределить на этапе запуска контейнера.

Есть две формы записи аргументов ENTRYPOINT и CMD: в виде строки и в виде массива строк. Первый вариант (так называемый shell режим) используется редко, поскольку не позволяет гибко настраивать работу образа. Обычно используется второй вариант (так называемый exec режим) — массив строк, который может состоять из команды и её параметров. Среди аргументов инструкции CMD строка с командой может и отсутствовать, если эта инструкция идёт после инструкции ENTRYPOINT. В этом случае строки массива рассматриваются как аргументы по умолчанию для команды, обозначенной в ENTRYPOINT.

**ENV. Переменные окружения** 

Переменные окружения задаются инструкцией ENV.

Через переменные окружения передают ключи и пароли к сервисам, режим работы, другие секретные и не очень значения. Например, запуск приложения Node.js для конечного пользователя обозначается дополнительной инструкцией:

```
FROM ubuntu:18.04
RUN sudo apt update && sudo apt install nodejs && sudo apt install npm
COPY . /app
ENV NODE_ENV=production
CMD ["node", "/app/app.js"]
```

**WORKDIR. Рабочая папка проекта** 

Инструкция WORKDIR задаёт рабочую папку приложения. Все инструкции в Dockerfile будут выполняться относительно неё.

Устанавливать рабочую папку — хороший тон. Она позволяет явно указать место, где будет происходить вся работа. Добавим её в нашу конфигурацию:

```
FROM ubuntu:18.04
RUN sudo apt update && sudo apt install nodejs && sudo apt install npm
WORKDIR /app
COPY . .
ENV NODE_ENV=production
CMD ["node", "app.js"]
```

**USER. Запуск от имени пользователя** 

Если приложение нужно запускать от имени пользователя системы, то используйте инструкцию USER с именем пользователя. Например, если вы хотите запускать приложение от имени пользователя node_user, то конфигурационный файл будет выглядеть так:

```
FROM ubuntu:18.04
RUN sudo apt update && sudo apt install nodejs && sudo apt install npm
WORKDIR /app
COPY . .
ENV NODE_ENV=production
USER node_user
CMD ["node", "app.js"]
```

**EXPOSE. Проброска порта вовне** 

Для запуска веб-приложения на компьютере вы используете веб-сервер, запущенный локально. Обычно веб-приложение становится доступным по адресу http://localhost:8080. Цифры в конце означают порт, открытый для запросов со стороны браузера или других приложений. Чтобы открыть в браузере веб-приложение, запущенное внутри контейнера, нужно «пробросить» запросы от браузера внутрь контейнера, а ответ от веб-приложения из контейнера наружу. Для этого используется перенаправление пакетов в виртуальном сетевом окружении (Docker Network):

Проброска портов образов Docker для веб-приложений
EXPOSE незаменим, когда в образе находится база данных и нам нужен доступ к ней вне контейнера. Для этого используется инструкция EXPOSE:

```
FROM ubuntu:18.04
RUN sudo apt update && sudo apt install nodejs && sudo apt install npm
WORKDIR /app
COPY . .
ENV NODE_ENV=production
USER node_user
EXPOSE 8080
CMD ["node", "app.js"]
```

Запись EXPOSE 8080 означает, что на компьютере, на котором запущен Docker, веб-приложение будет доступно по адресу http://localhost:8080.

**ARG. Аргументы командной строки** 

Во время сборки образа не всегда удобно, а иногда даже опасно, описывать все параметры внутри Dockerfile, поскольку этот файл обычно доступен в репозитории большинству разработчиков. В случае публичного репозитория это недопустимо вовсе. В этом случае следует пользоваться переменными, значения которых задаются на этапе сборки образа.

Передавать данные можно с помощью аргументов команды docker build на этапе сборки образа. Во время сборки эти аргументы можно использовать как переменные, достаточно их определить инструкцией ARG. Можно задать и значения по умолчанию на тот случай, если пользователь не укажет нужные аргументы. Например, передать имя пользователя внутрь контейнера можно следующим образом:

```
docker build --build-arg user=node_user .
```

В Dockerfile надо будет добавить соответствующие инструкции:

```
FROM ubuntu:18.04
RUN sudo apt update && sudo apt install nodejs && sudo apt install npm
WORKDIR /app
COPY . .
ENV NODE_ENV=production
# Значение по умолчанию 'deploy' (можно не указывать)
ARG user=deploy
USER $user
EXPOSE 8080
CMD ["node", "app.js"]
```

Важно, что так не следует передавать секретные данные, поскольку их можно будет увидеть в истории Docker:

```
docker history
```

Для безопасной передачи секретных данных лучше использовать тома Docker.

**Многоступенчатая сборка образа **

С точки зрения оптимизации сборки, уменьшения размера образа и ускорения приложения, образ можно собирать в несколько этапов. Например, с помощью платформы Node.js произвести сборку веб-приложения на первом этапе, а на втором — запустить готовый бандл с помощью веб-сервера. Операция копирования из первого промежуточного образа во второй целевой пройдёт совершенно незаметно. После сборки образ будет занимать мало дискового пространства, в нем будет все самое необходимое для работы веб-приложения:

```
# Сборка проекта на платформе Node.js
FROM node:lts-alpine as build-stage
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
RUN npm run build

# Запуск приложения на сервере
FROM nginx:stable-alpine as production-stage
COPY --from=build-stage /app/dist /usr/share/nginx/html
EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]
```

Имя промежуточного образа build-stage служит для передачи результата работы первой стадии сборки.

**Рекомендации** 

Для того чтобы использовать образы эффективнее, необходимо следовать рекомендациям от команды Docker:

Нужно создавать образы так, чтобы жизненным циклом контейнера можно было удобно управлять. Образ не должен хранить внутреннее состояние. Данные внутрь образа можно передать на этапе сборки с помощью аргументов командной строки, а на этапе работы контейнера можно пользоваться томами Docker.
Необходимо понимать контекст запуска веб-приложения: папка проекта, удалённый ресурс (remote source) или репозиторий.
Надо понимать, что Dockerfile может запускаться вне контекста через стандартный поток ввода.
Используйте файл .dockerignore для того, чтобы в образ попадали только нужные файлы и папки. От всего лишнего лучше избавиться на этапе сборки.
Используйте сборку приложения в несколько стадий. Это позволит существенно уменьшить размер образа.
Не устанавливайте то, что не будете использовать в образе.

Необходимо разделять приложения на обособленные части, которые способны выполняться независимо. Этот процесс носит название декаплинга (Decoupling).
Минимизируйте количество слоёв в образе. Это повышает производительность образа как при сборке, так и при работе контейнера.
Если параметры инструкции записываются в несколько строк (с помощью символа переноса строки \) необходимо выстраивать аргументы в алфавитном порядке. Это повышает читаемость файла и упрощает отладку.

Используйте кэш Docker только для тех слоёв, которые будут нужны для сборки других образов. Для этого достаточно добавить параметр --no-cache=true в команду сборки docker build.

**Сборка образа** 

Образ Docker можно собрать тремя способами:

– указав путь к папке PATH;
– указав путь к репозиторию URL;
– используя стандартный поток ввода –.

Чаще всего используется первый способ с указанием пути. Самая простая команда для сборки образа:

```
docker build .
```

С помощью этой команды собираться образ будет из текущей папки (. в конце), в которой должен быть Dockerfile.

**Использование нескольких Dockerfile** 

Иногда возникает необходимость использования нескольких вариантов сборок в одном проекте. В этом случае не обойтись без нескольких файлов с инструкциями. При сборке можно указать другое имя для файла конфигурации или относительный путь внутри PATH, нужно использовать флаг -f:

```
docker build -f containers/dockerfile-mode-1 .
```

Точно так же можно указать относительный путь для проекта или репозитория по некоторому URL. Например, Docker может скачать не только репозиторий GitHub, но и произвольный архив с проектом, распаковать его и собрать образ:

```
docker build -f ctx/Dockerfile http://server/ctx.tar.gz
```

Поддерживаются архивы форматов bzip2, gzip, xz.

Файлы и папки проекта, исполняемый файл приложения, архив или репозиторий Git составляют контекст образа. Но Docker позволяет собирать образы без контекста из стандартного потока ввода. Собрать такой образ можно командой:

```
docker build - < Dockerfile
```

Исключение файлов из сборки .dockerignore Секция статьи "Исключение файлов из сборки .dockerignore"
Если вам не нужно включать в образ какие-то папки или файлы из контекста, добавьте в папку файл исключений .dockerignore. В этом файле перечисляются в отдельных строках все пути или маски путей, которые не должны быть помещены в образ. Пример файла:

```
# Комментарий
*/temp*
*/*/temp*
temp?
```

– */temp позволяет не включать в образ файлы или папки, имена которых начинаются на temp, и которые находятся в любой папке первого уровня (например, /somedir/temporary.txt или /somedir/temp);
– */*/temp* — делает то же, но для папок второго уровня;
– temp? — позволяет не включать в образ файлы и папки из корневой папки образа, имена которых начинаются на temp и состоят из пяти символов, последний из которых может быть любым.

# 4.	Основные команды и создание контейнеров

## <a name="command"></a> 4.1.	Основные команды Docker


**Создать образ** — `**docker build -t**` название образа и путь до проекта. Например, команда`** docker build -t my_cont_app**` . соберёт образ под название **my_cont_ap**p из текущей директории (“.”).

Просмотреть список локальных образов —`** docker images**`.
Удалить образ — `**docker image rm -f**` название образа.
Удалить все образы — `**docker rmi**` $(docker images -q).
Поиск образа в удалённом репозитории — `** docker search**` имя образа.
Скачать образ из удалённого репозитория — `**docker pul**l` имя образа.
Загрузить образ в личный удаленный репозиторий на Docker hub — docker push название образа (можно начать вводить название и нажать tab).
Для загрузки образа на Docker hub — нужно выполнить следующие действия:

Собрать образ командой `**docker build -t**` и в качестве имени указать: логин Docker hub / название образа. Например: `**docker build -t ekdeus/acc-info**.`
Авторизоваться через консоль: docker login. Команда попросит ввести логин и пароль. После успешной авторизации можно переходить к загрузке образа.
Загрузить образ командой docker push имя образа. Например: `**docker push ekdeus/acc-info**`.

Запуск контейнера из образа осуществляется командой docker run с указанием разнообразных ключей и нужного образа. Базовый вид команды: docker run python:latest (любое имя образа). Вот некоторые полезные ключи команды docker run:

-t — предоставляет доступ к терминалу внутри контейнера.

-i — делает возможным взаимодействие с терминалом внутри контейнера.

-d — запуск контейнер в фоновом режиме. Это позволяет использовать терминал, из которого запущен контейнер, для выполнения других команд во время работы контейнера.

--rm — автоматическое удаление контейнера после завершения его работы.
Например, мы запускаем контейнер с Python с доступом к терминалу, команда будет такой: docker run -ti python:latest. Если не указать -ti, то выйти из терминала Python будет очень затруднительно.

Просмотреть список запущенных контейнеров можно командой docker ps. Однако в таком виде команда вернёт список только запущенных контейнеров, чтобы вывести список всех контейнеров, в том числе и остановленных, нужно указать ключ -a. Еще один полезный ключ для отображения списка контейнеров: -q. Он выводит только ID контейнеров.

Остановить контейнер можно командой docker stop имя/id контейнера, а перезагрузить — docker restart имя/id контейнера.

Для удаления контейнера применяется команда docker rm имя/id контейнера. Если нужно удалить все контейнеры можно применить связку команд: docker rm $(docker ps -qa).

Удалить все не связанные с контейнерами ресурсы (образы, контейнеры, тома и сети) можно командой docker system prune. Если требуется удалить все остановленные контейнеры и неиспользуемые образы можно воспользоваться командой docker system prune -a.

## 4.2.	Доступ в интернет внутри контейнеров

вы можете четко убедиться, что с Docker что-то не так, просто пропинговав google.comсвой хост и простой контейнер:

```
$ ping google.com
PING google.com (173.194.124.7) 56(84) bytes of data.
64 bytes from 173.194.124.7: icmp_req=1 ttl=56 time=11.1 ms
64 bytes from 173.194.124.7: icmp_req=2 ttl=56 time=13.2 ms
...

$ docker run -ti ubuntu ping google.com
# BLACK HOLE
```

Если вы застряли в такой ситуации, вы можете посмотреть, какой DNS-сервер Docker на самом деле использует, чтобы позволить вашим контейнерам разрешаться в Интернете.

В нашем случае мы поняли, что наш брандмауэр ведет себя довольно странно с общедоступными DNS-серверами Google ( 8.8.8.8и 8.8.4.4), которые, как оказалось, являются DNS-серверами по умолчанию, которые Docker будет использовать, если у вас нет ничего, указанного в вашем resolv.confи так далее.

В целом проблему было довольно легко обойти, так как мы только что сказали докеру использовать OpenDNS в нашем /etc/default/docker:

```
# Docker Upstart and SysVinit configuration file

# Use DOCKER_OPTS to modify the daemon startup options.

DOCKER_OPTS="--dns 208.67.222.222 --dns 208.67.220.220"
```

Затем вам нужно будет только перезапустить демон Docker, и все должно быть в порядке:

```
$ sudo service docker restart  
docker stop/waiting
docker start/running, process 26999

$ docker run ubuntu ping google.com   
PING google.com (173.194.124.2) 56(84) bytes of data.
64 bytes from 173.194.124.2: icmp_seq=1 ttl=55 time=11.4 ms
64 bytes from 173.194.124.2: icmp_seq=2 ttl=55 time=11.2 ms
...
```

## 4.3.	Создание контейнера NGINX



## 4.4.	Запуск контейнера в фоновом режиме

Foreground vs. Detached
Контейнер может работать на переднем плане, где он блокируется, пока процесс не завершится, и контейнер перестанет работать. В режиме переднего плана контейнер выводит свой вывод на консоль и может читать стандартный ввод. В отключенном режиме (когда вы указываете флаг -d), управление возвращается немедленно, а контейнер продолжает работать.

## 4.5.	Остановка контейнеров

Обычно контейнер завершается автоматически после завершения процесса, но иногда требуется собственноручно завершить запущенный контейнер. Команда stop осуществляет «мягкое» завершение контейнера, по умолчанию предоставляя 10 секунд для завершения всех процессов:

```
# docker stop КОНТЕЙНЕР
```

Для немедленного завершения выполните команду kill. В большинстве ситуаций stop предпочтительнее.

```
# docker kill КОНТЕЙНЕР
```

Немедленное завершение всех запущенных контейнеров:

```
# docker kill $(docker ps -q)
```

## 4.6.	Запуск дополнительных процессов в работающем контейнере



## 4.7.	Создание имени для контейнера

Использование идентификаторов контейнеров или автогенерированных имен иногда неудобно. Если вы часто взаимодействуете с контейнером, который вы часто повторно создаете, тогда он получит другой идентификатор и автогенерированное имя. Кроме того, имя будет случайным.

Docker позволяет вам называть свои контейнеры, когда вы запускаете их, предоставляя аргумент командной строки «--name <имя контейнера>». В простых случаях, когда у вас есть только один контейнер на образ, вы можете назвать контейнер по имени вашего образа: 

```
docker run --name hello-world hello-world.
```

Теперь, если мы посмотрим на процесс (я удалил clever_liskov ранее), мы увидим, что контейнер называется hello-world:

```
docker ps -a --format "table {{.ID}}\t{{.Names}}"
CONTAINER ID        NAMES
f6fe77b3b6e8        hello-world
```

Существует несколько преимуществ для именованного контейнера:

- У вас есть стабильное имя для ваших контейнеров, которые вы используете как в интерактивном режиме, так и в сценариях.
- Вы можете выбрать значащее имя.
- Вы можете выбрать краткое имя для удобства при работе в интерактивном режиме.
- Это предотвращает случайное наличие нескольких контейнеров одного и того же образа (при условии, что вы всегда указываете одно и то же имя).

Давайте посмотрим на последний вариант. Если я попытаюсь снова запустить ту же команду запуска с тем же именем «hello-world», я получаю сообщение об ошибке:

```
docker run --name hello-world hello-world
docker: Error response from daemon: Conflict. The container name
"/hello-world" is already in use by container 
f6fe77b3b6e8e77ccf346c32c599e67b2982893ca39f0415472c2949cacc4a51. 
You have to remove (or rename) that container to be able to reuse 
that name.
See 'docker run --help'.
```

## 4.8.	Выполнение команд в контейнере

Рассмотрим простейшие примеры выполнения произвольных команд внутри Docker контейнеров. Для этого сначала потребуется получить id контейнера. Оно возвращается первым столбцом при выполнении команды:

```
docker ps -a
```

Далее значение из столбца "CONTAINER ID" будет использовано для указания, в каком контенере необходимо выполнить произвольную bash команду:

```
docker exec -it id_контейнера bash_команда
```

К примеру, если id контейнера равен "fa8264bb1730" и нужно выполнить команду "ls -la" внутри контейнера, то:

```
docker exec -it fa8264bb1730 ls -la
```

В результате выполнения этой команды будет возвращён список файлов и папок в корневой директории Docker контейнера.

Если нужно открыть выполнить сразу несколько команд внутри контейнера, то будет целесообразно выполнить команду "bash" для вызова консоли:

```
docker exec -it fa8264bb1730 bash
```

После выполнения этой команды консоль откроется 
внутри контейнера. И можно будет вводить сразу несколько команд.

## 4.9.	Логи контейнеров

Многие знают, что есть команда docker logs. Чуть меньшее количество людей знает, что этот функционал можно расширить для отслеживания новых сообщений в контейнере (примерно как работает tail -f на обычных файлах), и уж совсем немногие знают и помнят, что можно указывать временные метки для просмотра логов контейнера только с определённого промежутка времени.

Самая простая команда для просмотра логов контейнера:

```
# docker logs mycontainer1
```

Так можно посмотреть все доступные логи контейнера mycontainer1, самые свежие сообещния будут в конце.

Отслеживание новых сообщений в логах Docker
Чтобы следить за появлением новых сообщений в контейнере, нужно указать опцию -f (от английского follow - следовать):

```
# docker logs -f mycontainer1
```

Сначала результат команды будет такой же, как и в предыдущем примере. Но во-первых, команда не завершится и вы не получите приглашения командной строки (это будет ждущий режим). А во-вторых, скоро должны появиться новые логи из контейнера.

Просмотр логов в промежутке времени
А вот это по-настоящему полезная весч. Можно получить только те сообщения в логах, которые произошли между указанными точками времени.

Для доступа к логам определённого рвемени, используйте параметры –since (от даты) и –until (до даты) для команды docker logs.

ЗАМЕЧУ: можно и не даты указвыать, а даты и время - но для моих нужд и дат хватает.

Вот пример просмотра логов контейнера для Java приложения. Как видите, благодаря моим параметрам мы получили только сообщения между 30м марта и 1м апреля:

```
root@s2:~ # docker logs -f confluence --since 2020-03-30 --until 2020-04-01
30-Mar-2020 07:52:27.292 INFO [http-nio-8090-exec-7] org.apache.coyote.http11.Http11Processor.service Error parsing HTTP request header
  Note: further occurrences of HTTP request parsing errors will be logged at DEBUG level.
     java.lang.IllegalArgumentException: Invalid character found in method name. HTTP method names must be tokens
         at org.apache.coyote.http11.Http11InputBuffer.parseRequestLine(Http11InputBuffer.java:415)
         at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:292)
         at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66)
         at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:861)
         at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1579)
         at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49)
         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
         at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
         at java.lang.Thread.run(Thread.java:748)
30-Mar-2020 09:22:28.099 INFO [http-nio-8090-exec-1] org.apache.coyote.http11.Http11Processor.service Error parsing HTTP request header
  Note: further occurrences of HTTP request parsing errors will be logged at DEBUG level.
     java.lang.IllegalArgumentException: Invalid character found in method name. HTTP method names must be tokens
         at org.apache.coyote.http11.Http11InputBuffer.parseRequestLine(Http11InputBuffer.java:415)
         at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:292)
         at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66)
         at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:861)
         at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1579)
         at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49)
         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
         at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
         at java.lang.Thread.run(Thread.java:748)
30-Mar-2020 23:57:24.900 INFO [http-nio-8090-exec-3] org.apache.coyote.http11.Http11Processor.service Error parsing HTTP request header
  Note: further occurrences of HTTP request parsing errors will be logged at DEBUG level.
     java.lang.IllegalArgumentException: Invalid character found in method name. HTTP method names must be tokens
         at org.apache.coyote.http11.Http11InputBuffer.parseRequestLine(Http11InputBuffer.java:415)
         at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:292)
         at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66)
         at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:861)
         at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1579)
         at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49)
         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
         at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
         at java.lang.Thread.run(Thread.java:748)
```

## 4.10.	Публикация портов контейнера

По умолчанию Docker запускает сервис изолированно — то есть все порты между хостом и Docker-контейнером закрыты. Для того, чтобы получить доступ к сервису с хоста или из внешней сети, нужно использовать сопоставление портов или маппинг.

Маппинг нужен для того, чтобы все запросы, проходящие через порт хоста, перенаправлялись в Docker-контейнер. Другими словами, сопоставление портов делает процессы внутри контейнера доступными извне.

При запуске нового Docker-контейнера с помощью команды docker run можно сопоставить порты опцией --publish или -p:

```
docker run -d -p 81:80 --name httpd-container httpd
```

Эта команда запускает Docker-контейнер httpd (HTTP-сервер Apache) и маппинг 81 порта хоста с 80 портом внутри Docker-контейнера. Стоит отметить, что по умолчанию сервер httpd прослушивает порт 80.

Теперь доступ к приложению можно получить, используя порт 81 на хосте:

```
curl http://localhost:81
<html><body><h1>It works!</h1></body></html>
```

Важно отметить, что выполнять маппинг для всех Docker-контейнеров необязательно. Например, в случаях, когда нужно сохранить службы Docker-контейнера закрытыми или видимыми только родственным контейнерам в той же сети Docker.

Как назначить маппинг работающему Docker-контейнеру
Рассмотрим ситуацию, в которой маппинг не был задан при запуске Docker-контейнера или было задан с ошибкой. Получить доступ к сервису через TCP/IP-соединение на хосте не получится, но есть три способа решить эту проблему:

1. Остановить работающий Docker-контейнер, а затем создать и запустить новый c оригинальным docker images
2. Сделать docker commit работающего Docker-контейнера, создать новый и запустите его через docker images, сохранив состояние
3. Добавить новый маппинг, используя файлы конфигурации Docker

Разберем каждый способ подробнее.

Перезапуск Docker-контейнера
Это самое технически простое решение: работающий Docker-контейнер удаляется, а вместо него создается новый, запущенный из образа. При создании Docker-контейнера сопоставляются порты.

Хотя это самый простой способ, он подходит не во всех ситуациях. Например, решение не подойдет в ситуации, когда внутри Docker-контейнера уже выполнены некоторые операции: установились зависимости, запустились процессы или обновились файлы. Если восстановить такой Docker-контейнер из образа, все эти изменения будут потеряны.

Стоит отметить, что важные данные лучше хранить в Docker volumes, чтобы упростить замену Docker-контейнеров.

Повторный запуск из docker commit
Этот подход предполагает создание нового образа на базе существующего Docker-контейнера. Затем образ можно использовать для запуска нового Docker-контейнера с открытыми нужными портами.

Поскольку мы фиксируем состояние существующего Docker-контейнера оно становится доступно в новом образе.

Попробуем реализовать этот способ. Сначала остановим Docker-контейнер и создадим его образ с помощью команды docker commit:

```
# Останавливаем контейнер
docker stop httpd-container
httpd-container
# Создаем образ на основа существующего контейнера
docker commit httpd-container httpd-image
# Хэш образа
sha256:33da33fcad051c90ac9b7dea9b2dbda442767e05ddebd8d6db8ac6893ef4ef40
```

Затем удалим Docker-контейнер и заново запустим его из образа. На этом этапе важно убедиться, что маппинг настроен верно:

```
# Удаляем контейнер
docker rm httpd-container
# Команда напечатает на экране название удалённого контейнера
httpd-container
# Запускаем контейнер
docker run -d -p 83:80 --name httpd-container httpd-image
# На экране выведится хеш запущенного контейнера
dd2535c477ad74e80b3642abca9055efacb89eaf14572b91f91bf20cd3f0cbf3
```

Перенастройка Docker-контейнера без его удаления
В подходах, которые обсуждались выше, речь шла об удалении существующего Docker-контейнера и создании нового. Хотя новый Docker-контейнер работает также, его метаданные полностью отличаются.

Рассмотрим способ, как настроить маппинг в уже существующем Docker-контейнере. Сначала запустим новый его без сопоставления портов:

```
docker run -d --name httpd-container httpd 
a0ed1c9fc60c358d53400dc244e94ef0db4d1347e70bd4be725a890b062ebbe7
# Команда выведет запущенные контейнеры
docker ps
CONTAINER ID   IMAGE     COMMAND              CREATED             STATUS          PORTS       NAMES
a0ed1c9fc60c   httpd     "httpd-foreground"   1 second ago        Up 1 second     80/tcp      httpd-container
```

Команда docker run возвращает полный идентификатор Docker длиной 64 символа. Другой способ получить его — команда docker inspect:

```
$ docker inspect --format="{{.Id}}" httpd-container
a0ed1c9fc60c358d53400dc244e94ef0db4d1347e70bd4be725a890b062ebbe7
```

Идентификатор пригодится для поиска файла конфигурации Docker.

Остановка Docker-контейнера и службы Docker
Первый шаг по перенастройке работающего Docker-контейнера — его остановка. Выполнить ее можно командой docker stop:

```
docker stop httpd-container
httpd-container
```

Для обновления конфигурационных файлов Docker-контейнера нужно остановить и службу Docker. Стоит отметить, что это действие приведет к отключению всех Docker-контейнеров до тех пор, пока процесс настройки не закончится.

```
# Останавливаем службу Docker
systemctl stop docker
```

Как найти конфигурационные файлы
Все конфигурационные файлы, которые относятся к Docker-контейнерам, образам, томам и сетям Docker, можно найти в каталоге /var/lib/docker. Стоит отметить, что на Windows и MacOS путь к каталогу отличается.

В данном случае интерес представляют два файла: hostconfig.json и config.v2.json. Найти их можно в следующем каталоге:

```
/var/lib/docker/containers/<ID>/
```

В данном случае ID — полный идентификатор Docker-контейнера, который мы получили ранее:

```
a0ed1c9fc60c358d53400dc244e94ef0db4d1347e70bd4be725a890b062ebbe7
```

Конфигурационные файлы можно найти в следующем каталоге:

```
$ ls /var/lib/docker/containers/a0ed1c9fc60c358d53400dc244e94ef0db4d1347e70bd4be725a890b062ebbe7/
a0ed1c9fc60c358d53400dc244e94ef0db4d1347e70bd4be725a890b062ebbe7-json.log  checkpoints  config.v2.json  hostconfig.json  hostname  hosts  mounts  resolv.conf  resolv.conf.hash
```

Обновление конфигурационных файлов
В первую очередь стоит обновить файл hostconfig.json. Для этого найдем ключ привязки портов в этом файле. Поскольку при запуске Docker-контейнера маппинг не был настроен, поле ключа PortBindings Docker-контейнера будет пустым:

```
{
  ...
  ...
  "PortBindings": {},
  ...
  ...
}
```

Следующий шаг — назначение 82 порта хоста 80 порту Docker-контейнера httpd-container. Для этого обновим привязки портов в файле JSON:

```
{
  ...
  ...
  "PortBindings": {"80/tcp":[{"HostIp":"","HostPort":"82"}]},
  ...
  ...
}
```

Как только маппинг выполнен, нужно открыть порт 80 Docker-контейнера в файле config.v2.json. Для этого добавим поле открытых портов (если его еще нет) в ключ конфигурации:

```
{
  ...
  "Config":
  {
    ...
    "ExposedPorts":
    {
      "80/tcp":{}
    },
    ...
  }
}
```

Таким образом мы сообщаем демону Docker, что Docker-контейнер прослушивает 80 порт в этом сетевом интерфейсе.

Можно открыть несколько портов, передав значения в виде пар ключ-значение, разделенных запятыми:

```
{
  ...
  "Config":
  {
    ...
    "ExposedPorts":
    {
      "80/tcp":{},
      "82/tcp":{},
      "8080/tcp":{}
    },
    ...
  }
}
```

Проверка изменений
Проверка изменений состоит из нескольких шагов. Сначала запустим службу Docker:

```
systemctl start docker
```

Теперь запустим httpd-контейнер и проверим маппинг портов с помощью команды docker ps:

```
docker start httpd-container
httpd-container
docker ps
CONTAINER ID   IMAGE     COMMAND              CREATED             STATUS          PORTS                               NAMES
a0ed1c9fc60c   httpd     "httpd-foreground"   1 hours ago         Up 1 seconds    0.0.0.0:82->80/tcp, :::82->80/tcp   httpd-container
```

Из результата выполнения команды видно, что порты Docker-контейнера теперь сопоставлены с портами хоста. После этого доступ к службе httpd Docker-контейнера можно получить извне — через 82 порт:

```
curl http://localhost:82
<html><body><h1>It works!</h1></body></html>
```

При таком подходе идентификатор Docker-контейнера и все выполненные в нем настройки сохраняются, поскольку исходный контейнер не заменяется новым.

Изменить существующие маппинги любого Docker-контейнера можно используя процедуру, описанную выше. Единственное отличие будет заключаться в том, что ключ привязки портов в файле hostconfig.json не будет пустым. В этом случае достаточно просто обновить номер используемого порта.

При обновлении TCP-порта Docker-контейнера в маппинге необходимо указать тот же порт в файле config.v2.json.

Теперь все проверки прошли — можно запустить службу Docker и Docker-контейнер, чтобы изменения вступили в силу.

## 4.11.	Разные порты для разных контейнеров

По сути, большинство контейнеров Docker имеют свои порты, настроенные в формате host:container.

Например, 5000:80 забирает порт 80 контейнера и делает его доступным с порта 5000 хоста.

Однако на самом деле вы можете связывать сокеты напрямую, то есть вместо 5000 вы можете подставить пару IP:PORT, используя три двоеточия для всей привязки:

```
docker run -it -d ipaddress:hostport:containerport --name web nginx
```

Например, вы можете иметь два контейнера NGINX на разных IP-адресах, например, так (запомните, что Docker требует отдельных имен для контейнеров):

```
docker run -it -d 123.0.0.1:80:80 --name web nginx
docker run -it -d 123.0.0.2:80:80 --name web2 nginx
```

Если вы используете Docker Compose, конфигурация будет аналогичной.

В разделе портов для службы вы можете использовать тот же синтаксис для привязки к определенным адресам.

```
version: "3"
services:
  nginx:
    image: nginx
    restart: always  
    ports: 
      - "123.0.0.1:80:80"
```

В любом случае, вы можете создать несколько сервисов, привязанных к порту хоста 80, при условии, что сервисы не прослушивают одни и те же IP-адреса.

## 4.12.	Автоматическое удаление остановленных контейнеров

По умолчанию контейнеры остаются в работе. Иногда они вам не нужны. Вместо того, чтобы вручную удалять остановленные контейнеры, вы можете это автоматизировать. Для этого нужно добавить аргумент командной строки `--rm`.
Контейнер, запущенный с аргументом `--rm` будет удален после остановки.

```
docker run --rm hello-world
```

## 4.13.	Разделение команды на строки

`\` используется для разделения длинных команд на несколько строк

Следующая команда запускает контейнер postgres:

```
docker run --rm \
 # название контейнера
 --name postgres \
 # пользователь
 -e POSTGRES_USER=postgres \
 # пароль
 -e POSTGRES_PASSWORD=postgres \
 # название базы данных
 -e POSTGRES_DB=mydb \
 # автономный режим и порт
 -dp 5432:5432 \
 # том для хранения данных
 -v $HOME/docker/volumes/postgres:/var/lib/postgresql/data \
 # образ
 postgres
```

# 5.	Управление контейнерами

## 5.1.	Просмотр статистики контейнеров

Отслеживание статистики с помощью docker stats
Для отслеживания метрик среды выполнения контейнера в режиме реального времени можно использовать команду docker stats. Команда отображает статистику использования ресурсов контейнерами и поддерживает такие метрики, как утилизация CPU, памяти, ограничения на использование памяти, а так же метрики для сети и блочного IO.

Ниже приведен пример вывода команды docker stats:

```
$ docker stats redis1 redis2

CONTAINER           CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O
redis1              0.07%               796 KB / 64 MB        1.21%               788 B / 648 B       3.568 MB / 512 KB
redis2              0.07%               2.746 MB / 64 MB      4.29%               1.266 KB / 648 B    12.4 MB / 0 B
```

Более подробно о команде вы сможете прочитать на странице docker stats

**Контрольные группы**

Контейнеры в Linux реализованы с использованием контрольных групп, которые не только позволяют задать ограничения для групп процессов, но также предоставляют метрики использования CPU, памяти, блочного ввода-вывода. Помимо этих метрик, можно также получить статистику использования сети. Все это относится как к контейнерам LXC, так и к контейнерам Docker.

Контрольные группы реализуются через специальную псевдофайловую систему. В последних версиях дистрибутивов вы можете найти данную файловую систему в каталоге /sys/fs/cgroup, для некоторых дистрибутивов может использоваться другой путь. В этой файловой системе вы увидите множество подкаталогов, которые называются devices, freezer, blkio и другие; каждый подкаталог соответствует отдельной иерархии контрольных групп.

Чтобы найти точку монтирования, к которой у вас подключены контрольные группы, воспользуйтесь командой:

```
$ grep cgroup /proc/mounts
```

**Перечисление cgroups**

Чтобы определить известные системе подсистемы контрольных групп и статистику групп в них, просмотрите файл /proc/cgroups.

```
$ cat /proc/cgroup

#subsys_name	hierarchy	num_cgroups	enabled
cpuset	         2	            1	       1
cpu	             3	          122	       1
cpuacct	         3	          122	       1
blkio	        10	          122	       1
memory	        11	          226	       1
devices	         8	          123	       1
freezer	         6	            2          1
net_cls	        12	            1	       1
perf_event	     7	            1	       1
net_prio        12	            1	       1
hugetlb	         9	            1	       1
pids	         4	          134	       1
rdma	         5	            1	       1
```

Вы можете просмотреть файл /proc/<pid>/cgroup, чтобы определить, к каким контрольным группам принадлежит процесс. Контрольная группа отображается как путь от директории монтирования иерархии. / означает, что процесс не был назначен конкретной группе, а /lxc/pumpkin указывает на то, что процесс принадлежит контейнеру pumpkin.

Поиск контрольной группы конкретного контейнера
Для каждого контейнера в каждой иерархии создается одна cgroup. В ранних системах с более ранними версиями пользовательских утилит LXC, имя контейнера является именем контрольной группы. В более поздних версиях инструментов LXC, имя группы соответствует lxc/<container_name>.

Для контейнеров Docker, именем контейнера является полный ID контейнера. Если контейнер отображается в docker ps как ae836c95b4c3, его полный ID может быть, например, таким: ae836c95b4c3c9e9179e0e91015512da89fdec91612f63cebae57df9a5444c79. Его можно узнать с помощью docker inspect docker ps --no-trunc или docker ps --no-trunc.

Если обобщить все сказанное выше, то, к примеру, метрики памяти контейнера Docker находятся в каталоге /sys/fs/cgroup/memory/docker/<longid>/.

Метрики из cgroup: память, CPU, блочный I/O
Для каждой подсистемы (память, CPU и блок ввода-вывода) есть один или несколько псевдофайлов со статистикой.

**Метрики памяти**

Метрики памяти находятся в контрольной группе “memory”. Эта группа создает дополнительную небольшую нагрузку на систему, потому что производит подробный учет использования памяти на вашем хосте. Таким образом, во многих дистрибутивах ее по умолчанию отключают. Как правило, чтобы ее включить, достаточно добавить параметры командной строки ядра: cgroup_enable=memory swapaccount=1.

Метрики сохраняются в псевдофайл memory.stat и выглядят следующим образом:

cache 11492564992
rss 1930993664
mapped_file 306728960
pgpgin 406632648
pgpgout 403355412
swap 0
pgfault 728281223
pgmajfault 1724
inactive_anon 46608384
active_anon 1884520448
inactive_file 7003344896
active_file 4489052160
unevictable 32768
hierarchical_memory_limit 9223372036854775807
hierarchical_memsw_limit 9223372036854775807
total_cache 11492564992
total_rss 1930993664
total_mapped_file 306728960
total_pgpgin 406632648
total_pgpgout 403355412
total_swap 0
total_pgfault 728281223
total_pgmajfault 1724
total_inactive_anon 46608384
total_active_anon 1884520448
total_inactive_file 7003344896
total_active_file 4489052160
total_unevictable 32768

Первая половина показателей, без префикса total_, содержит статистику процессов, относящихся к контрольной группе, не включая подгруппы. Вторая половина метрик, с префиксом total_, включает в себя итоговую статистику, включая подгруппы.

Некоторые показатели отражают текущие значения, то есть могут увеличиваться или уменьшаться. Например, swap – размер файла подкачки, используемого участниками контрольной группы. Другие показатели являются “счетчиками”, то есть значениями, которые могут только увеличиваться и сбрасываться, потому что они подсчитывают некоторые события. Например, pgfault указывает количество промахов страниц виртуальной памяти, которые произошли с момента создания группы.

**Метрики CPU**

Метрики использования ресурсов процессора для каждого контейнера содержится в псевдофайле cpuacct.stat. Они аккумулируют использование процессора всеми процессами контейнера и разбиты по времени по нахождению процессов в пространстве пользователя и пространстве ядра:

user – период времени, в течение которого процесс использует процессор для выполнения непривилегированного кода процесса;
system – период времени, в течение которого ядро выполняет системные вызовы от имени процесса.
Время подсчитывается в единицах, равных 1/100 секунды, также известных как jiffies или тики. Количество тиков в секунду определяется переменной USER_HZ, и в системах x86 USER_HZ равен 100. Раньше это количество соответствовало количеству тиков планировщика в секунду, но с повышением частоты и появлением бестиковых ядер показатель стал полностью синтетическим.

**Метрики блочного I/O**

Метрики блочного ввода/вывода учитываются в подсистеме blkio. В разных файлах сохраняются разные метрики. Подробнее о них можно узнать из документации ядра. Ниже представлен краткий список наиболее часто используемых метрик:
Метрика	Описание
blkio.sectors	Содержит количество 512-байтовых секторов, считанных и записанных процессами контрольной группы для каждого устройства. Операции чтения и записи учитываются в одном показателе.
blkio.io_service_bytes	Определяет количество байт, считанных и записанных контрольной группой. Имеет 4 счетчика для каждого устройства, поскольку для каждого устройства отдельно учитываются синхронные и асинхронные операции ввода/вывода, а также операции чтения и записи.
blkio.io_serviced	Число произведенных операций ввода/вывода, вне зависимости от их размера. Также имеет 4 счетчика для каждого устройства.
blkio.io_queued	Определяет количество запросов ввода/вывода, инициированных контрольной группой, которые на данный момент поставлены в очередь.

**Метрики сети**

Метрики сети напрямую не учитываются контрольными группами. Этому есть хорошее объяснение: сетевые интерфейсы существуют в контексте сетевых пространств имен. Ядро могло бы собирать метрики о количестве пакетов и байтов, отправленных и полученных группой процессов, но эти показатели были бы бесполезны. Пользователю обычно нужны метрики по каждому интерфейсу. Но так как процессы в отдельной контрольной группе могут относиться к множеству сетевых пространств имен, эти показатели будет сложно интерпретировать – множество сетевых пространств имен означает множество интерфейсов lo, множество интерфейсов eth0 и так далее; поэтому собрать показатели сети с контрольных групп нелегко.

Вместо этого можно собирать метрики сети из других источников.

IPtables
Достаточно подробный учет могут делать IPtables (или фреймворк netfilter, для которого iptables является просто интерфейсом).

Например, можно задать правило учета исходящего HTTP-трафика на веб-сервере:

```
$ iptables -I OUTPUT -p tcp --sport 80
```

Здесь нет флага -j или -g, поэтому правило считает только соответствующие пакеты и переходит к следующему правилу.

Затем, можно проверить значения счетчиков с помощью:

```
$ iptables -nxvL OUTPUT
```

Счетчики учитывают пакеты и байты. Если необходимо задать такие метрики для трафика контейнера, можно выполнить цикл for, чтобы добавить две цепочки iptables для каждого IP-адреса контейнера в цепочке FORWARD.

Затем нужно периодически проверять эти счетчики. Если вы используете collectd, есть хороший плагин для автоматизации сбора учетных данных с IPtables.

Счетчики на уровне интерфейса
Так как каждый контейнер имеет виртуальный интерфейс Ethernet, возможно, вам захочется проверить напрямую счетчики TX и RX этого интерфейса. Каждый контейнер в вашем хосте связан с виртуальным интерфейсом Ethernet с именем, например, vethKk8Zqi. К сожалению, сложно определить, какой интерфейс к какому контейнеру относится.

На сегодня лучше всего проверять метрики из контейнеров. Для этого вы можете запустить исполнительный файл из среды хоста в сетевом пространстве имен контейнера, используя ip-netns.

Команда ip-netns exec позволяет выполнить программу в любом сетевом пространстве имен, доступном текущему процессу. Это значит, что ваш хост может войти в сетевое пространство имен контейнера.

Формат команды следующий:

```
$ ip netns exec <nsname> <command...>
```

Например:

```
$ ip netns exec mycontainer netstat -i
```

ip-netns находит контейнер mycontainer с помощью псевдофайлов пространств имен. Каждый процесс относится к одному сетевому пространству имен, одному пространству имен PID, одному пространству имен mnt и так далее. Эти пространства имен описываются в /proc/<pid>/ns/*. Например, сетевое пространство имен PID 42 отображается в псевдофайл /proc/42/ns/net.

При запуске ip netns exec mycontainer ... ожидается, что /var/run/netns/mycontainer будет одним из таких псевдофайлов. (Допускаются ссылки).

Иными словами, для выполнения команды внутри сетевого пространства имен контейнера необходимо:

Определить PID любого процесса в контейнере, данные которого мы хотим получить;
Создать ссылку из /var/run/netns/<somename> к /proc/<thepid>/ns/net;
Выполнить ip netns exec <somename> .....
Заново просмотрите раздел «Перечисление групп» для того чтобы понять, как найти контрольную группу, для которой вы хотите хотите измерить статистику сети. Затем вы можете просмотреть псевдофайл с именем «tasks» в контрольной группе, который содержит все PID в группе (и, следовательно, в контейнере). Выберите любой из PID.

В общем, если “короткий ID” контейнера содержится в переменной окружения $CID, то можно сделать следующее:

```
$ TASKS=/sys/fs/cgroup/devices/docker/$CID*/tasks
$ PID=$(head -n 1 $TASKS)
$ mkdir -p /var/run/netns
$ ln -sf /proc/$PID/ns/net /var/run/netns/$CID
$ ip netns exec $CID netstat -i
```

**Советы для более эффективного сбора статистики**

Запускать новый процесс каждый раз, когда нужно обновить показатели, может быть довольно затратно. Если вы хотите собирать показатели с большой частотой и/или с большого количества контейнеров (например, 1000 контейнеров на одном хосте), не следует каждый раз создавать новый процесс.

Вы можете собирать метрики с помощью одного процесса, написав обработчик для сбора метрик на С или любом другом языке, который дает возможность выполнять низкоуровневые системные вызовы. Необходимо использовать специальный системный вызов, setns(), который позволит текущему процессу попасть в пространство имен. Для данного системного вызова необходимо наличие открытого файлового дескриптора для псевдофайла пространства имен (речь идет о псевдофайле в /proc/<pid>/ns/net).

Не оставляйте дескриптор открытым после сбора метрик, поскольку пока существует последний процесс контрольной группы, будут существовать и пространство имен, а его сетевые ресурсы (например, виртуальный интерфейс контейнера) не будут освобождены.

Правильнее каждый раз при сборе статистики для некоторого контейнера заново открывать псевдофайл пространства имен при необходимости.

Сбор метрик после завершения контейнера
Иногда нет необходимости отслеживать метрики в режиме реального времени, но когда контейнер уже остановлен, требуется понять сколько CPU, памяти и других ресурсов он использовал.

Для Docker это довольно сложно реализовать, что связано с использованием lxc-start, которая тщательно удаляет все, относящееся к контейнеру после вызова. Обычно проще собирать метрики через равные промежутки времени. Именно так работает LXC-плагин collectd. Но если все же необходимо собрать статистику после остановки контейнера, то это можно сделать так:

Для каждого контейнера запустите процесс сбора статистики, и переместите его в те контрольные группы, которые вы хотите отслеживать, записав PID процесса в файл tasks контрольной группы. Процесс сбора метрик должен периодически перечитывать файл tasks, чтобы проверить, является ли данный процесс последним в контрольной группе. Если нужно также собрать статистику сети, как описано в предыдущем разделе, то необходимо перенести процесс в соответствующее сетевое пространство имен.

Когда контейнер будет остановлен, lxc-start попытается удалить контрольные группы. У нее это не получится это сделать, так как контрольные группы еще используются нашим процессом сбора метрик. Теперь ваш процесс должен определить, что он является единственным процессом, оставшимся в группе, и собрать метрики для контейнера.

Наконец, процесс должен вернуться в корневую контрольную группу и удалить контрольную группу контейнера. Чтобы ее удалить, используйте rmdir. После очистки можно безопасно завершить процесс сбора статистики.

## 5.2.	Автоматический запуск контейнеров

Docker поддерживает автоматический перезапуск контейнеров. Для этого нужно создать или запустить контейнер с ключом `--restart`. Возможные значения ключа:

```
no -- без автоматического перезапуска;
on-failure -- перезапускать контейнер, если он остановился по ошибке (ненулевой код ошибки);
unless-stopped -- перезапускать контейнер, пока он не будет остановлен явно (командой stop) или пока не завершится демон Docker;
always -- всегда перезапускать контейнер.
```

Пример:

```
docker run -dit --restart unless-stopped redis
```

```
docker create -t -i -p 8080:8080 -v /var/lib/youtrack:/var/lib/youtrack -v /var/log/youtrack:/var/log/youtrack -v /etc/youtrack:/usr/local/youtrack/conf -v /tmp:/tmp --name docker-youtrack --restart unless-stopped dzwicker/docker-youtrack
```

## 5.3.	Просмотр событий в Docker

Docker Engine регистрирует событие каждый раз, когда демон выполняет какие-либо значительные действия.

Вы можете обратиться к журналу событий, чтобы определить, когда произошло то или иное действие, и отследить изменения объектов с течением времени.

В этой статье мы объясним, что фиксируется как события и как их просмотреть.

Затем мы покажем, как отслеживать события в режиме реального времени с помощью Docker CLI и REST API.

Docker Engine регистрирует событие каждый раз, когда демон выполняет значительные действия.

Вы можете обратиться к журналу событий, чтобы определить, когда произошло то или иное действие, и отследить изменения объектов с течением времени.

В этой статье мы объясним, что фиксируется как события и когда вы можете захотеть их просмотреть.

Затем мы покажем, как отслеживать события в режиме реального времени с помощью Docker CLI и REST API.

Большинство взаимодействий с такими объектами, как контейнеры, образы, тома и сети, записывают событие, создавая журнал, который можно использовать для анализа прошлых изменений.

Существует множество различных видов событий, которые определяют конкретные изменения в вашей среде:

Создание и удаление контейнеров
Статусы проверки работоспособности контейнеров
Команды, выполняемые внутри контейнеров с помощью docker exec
Пуш и пул образов
Создание, уничтожение, монтирование и размонтирование томов
Включение и отключение плагинов демонов Docker.
Полный список можно посмотреть в документации Docker.
Каждое записанное событие содержит метку времени и идентификатор затронутого объекта.

Вы можете использовать эту информацию для сбора истории изменений в вашей среде, независимо от того, наблюдали ли вы их первоначальные триггеры.

Сохраненные события также могут помочь в диагностике таких проблем, как неожиданные сбои контейнеров.

Просмотр лога позволяет определить точное время остановки контейнера, предоставляя дата поинт, который можно соотнести с другими журналами.

События могут установить, когда проверки контейнера начали давать сбои, сужая период, когда вам нужно проверить внешние службы, чтобы определить основную причину проблемы.

Передача событий Docker с помощью Docker CLI
Команда docker events CLI транслирует события от вашего демона Docker в окно терминала.

События будут появляться в реальном времени до тех пор, пока вы не завершите процесс нажатием комбинации клавиш Ctrl+C.

docker cli
Выполнение команды без аргументов не покажет никаких результатов.

Отображается только новая активность, поэтому вывод остается пустым до тех пор, пока не произойдет событие.

Вы можете спровоцировать его, запустив новый контейнер в другой оболочке:

docker run --rm hello-wor
Теперь в окне терминала, в котором запущена команда docker events, должно появиться несколько событий:

```
2022-05-31T15:20:00.267970018+01:00 image pull hello-world:latest (name=hello-world)
2022-05-31T15:20:00.347054862+01:00 container create 4a6c8d34a183363db5dbfdcc3cab4c82c4a341d719df56ec2e7f879ee8f02378 (image=hello-world, name=nifty_morse)
2022-05-31T15:20:00.347805277+01:00 container attach 4a6c8d34a183363db5dbfdcc3cab4c82c4a341d719df56ec2e7f879ee8f02378 (image=hello-world, name=nifty_morse)
2022-05-31T15:20:00.621070053+01:00 container start 4a6c8d34a183363db5dbfdcc3cab4c82c4a341d719df56ec2e7f879ee8f02378 (image=hello-world, name=nifty_morse)
...
```

Каждое событие отображается в отдельной строке.

Сначала отображается временная метка события, затем тип затронутого объекта (например, образ или контейнер), а затем действие, которое было выполнено (например, создание, присоединение и запуск).

Оставшаяся часть сообщения содержит полезные метаданные об объекте.

Приведенный выше пример показывает, что был извлечен образ hello-world:latest и на его основе создан контейнер.

Форматирование вывода
Необработанный список событий часто бывает громоздким.

Вы можете изменить формат вывода с помощью флага –format, который принимает строку шаблона Go:

```
docker events --format '{{ .Time }} {{ .Action }} {{ .Type}} {{ .ID }}'
```

Выполнение этого примера даст результат, который выглядит следующим образом:

```
1654006800 pull image hello-world:latest
1654006800 create container 4a6c8d34a183363db5dbfdcc3cab4c82c4a341d719df56ec2e7f879ee8f02378
1654006800 attach container 4a6c8d34a183363db5dbfdcc3cab4c82c4a341d719df56ec2e7f879ee8f02378
1654006800 start container 4a6c8d34a183363db5dbfdcc3cab4c82c4a341d719df56ec2e7f879ee8f02378
```

Вы можете получать события, представленные в виде объектов JSON, используя {{ json . }} в качестве строки шаблона:

```
docker events --format '{{ json . }}' | 
```

Здесь необработанный JSON передается через jq, так что он будет выведен в терминале.

☠ Как анализировать и вывести JSON с помощью инструментов командной строки Linux

Это облегчает сканирование информации.

В большинстве случаев вам нужно будет писать первую букву каждого свойства с заглавной буквы, например, time – {{ .Time }}.

Фильтрация событий
Журнал событий для загруженного демона Docker может быстро стать шумным.

Вы можете сузить круг событий до определенного действия, объекта или типа объекта с помощью флага –filter:

docker events –filter type=container – Получить все события, относящиеся к контейнерам.
docker events –filter event=create – Получить события создания контейнера.
docker events –filter container=demo-container – Получить все события, сохраненные для контейнера под названием demo-container (можно сослаться на ID или имя контейнера).
Кроме контейнера, вы можете фильтровать по всем поддерживаемым именам типов объектов, таким как образ, сеть и том.

При повторении флага –filter поддерживается несколько фильтров.

Различные фильтры интерпретируются как логические условия AND; несколько применений одного и того же фильтра становятся условиями OR.

Вот пример, в котором событие create используется для контейнеров app-container и api-container:

docke
Доступ к историческим событиям
По умолчанию docker events показывает только события, сохраненные с момента выполнения команды.

Вы можете включить исторические события, добавив флаг –since.

Этот флаг принимает человекочитаемое выражение времени или абсолютную временную метку:

```
docker events --since 1h
docker events --since '2021-05-01T16:00:00'
```

События, записанные после указанного времени, будут немедленно показаны в вашем терминале.

Новые события будут продолжать отображаться в реальном времени по мере их записи.

Вы можете исключить события после определенного времени с помощью флага –until.

Он работает аналогично –since.

Использование –until отключает потоковую передачу новых событий в реальном времени, поскольку они выходят за пределы запрашиваемого периода времени.

Передача событий Docker из REST API демона
Другой способ доступа к сохраненным событиям – это REST API демона Docker.

Вы можете использовать /events для потоковой передачи событий в реальном времени после включения API на вашем хосте Docker.

События будут возвращены в формате JSON:

```
curl http://127.0.0.1:2375/v1.41/events
{
"Type": "container",
"Action": "create",
"Actor": {
"ID": "4a6c8d34a183363db5dbfdcc3cab4c82c4a341d719df56ec2e7f879ee8f02378",
"Attributes": {
"image": "hello-world",
"name": "nifty_morse"
}
},
"scope": "local",
"time": 1654006800,
"timeNano": 1654006800347054800
```

Конечная точка API поддерживает параметры filter, since и until, которые имеют такое же поведение, как и их аналоги в CLI.

Вот как получить все события создания контейнера, записанные за последний час:

```
curl http://127.0.0.1:2375/v1.41/events?since=1h&filters={'type':'container','action':'create'}
```

Отправка событийyf внешнюю службу
В Docker отсутствует встроенный способ отправки событий во внешнюю службу.

Это может быть полезно, если вы хотите, чтобы все созданные вами контейнеры регистрировались в существующей платформе мониторинга или аудита.

Вы можете создать собственное решение, создав системную службу, которая постоянно запускает события docker.

Docker должен отправлять каждую новую строку вывода в вашу внешнюю систему.

Сначала напишем скрипт Bash, который реализует необходимую вам функциональность:

```
!/bin/bash
docker events --format '{{ .Time }} {{ .Action }} {{ .Type }} {{ .ID }}' | while read event
do
curl \
-X POST \
-H "Content-Type: application/json" \
-d '{"event": "$event"}' \
https://example.com/events
done
```

Теперь создайте новый юнит службы systemd по адресу /etc/systemd/system/docker-events.service:

```
[Unit]
Description=Custom Docker Event Monitoring Service

[Service]
Type=forking
ExecStart=/usr/local/bin/docker-events.sh

[Install]
WantedBy=multi-user.target
```

Наконец, перезагрузите systemd, чтобы загрузить вашу службу, затем запустите и включите юнит:

```
$ sudo systemctl daemon-reload
$ sudo systemctl start docker-events
$ sudo systemctl enable docker-events
```

Теперь ваша служба будет передавать каждое новое событие на вашу платформу мониторинга.

Включение службы настраивает ее на автоматический запуск при каждой перезагрузке хоста.

Заключение
Cобытия Docker создаются каждый раз, когда демон изменяет объекты в вашей среде.

Потоковая передача лога событий позволяет отслеживать активность демона в режиме реального времени.

Это может помочь вам в отладке проблем, аудите изменений и обеспечении соответствия требованиям.

## 5.4.	Управление остановленными контейнерами



## 5.5.	Portainer

Portainer – это проект с открытым исходным кодом, предоставляющий вам минимальный по размеру (3 Мб) образ Web-интерфейса для управления Docker Engine или Swarm кластером.
Основная панель
Как видно из экрана, Portainer — это отличное решение, которое позволяет не только очень наглядно предоставлять полную информацию по вашим Docker хостам и кластерам, но и эффективно управлять ими, предоставляя вам множество возможностей, вкратце о которых ниже.


Сервисы (Services)
Если ваш Docker Engine запущен в Swarm mode, вам становится доступно меню управления сервисами Swarm кластера. Процесс создания сервиса продемонстрирован на экране ниже. Прямо из Web-UI вы можете задать все значимые параметры сервиса:

Имя самого сервиса
Имя образа, из которого будет запущен сервис
Указать сторонний Docker реестр
Режим работы планировщика
Мапинг портов, дисков, сетей
А также метки
Контейнеры (Containers)
Помимо управления службами Portainer дает возможность управлять жизненным циклом контейнеров, запущенных на вашем хосте или кластере. Интерфейс создания контейнера ничем не отличается от интерфейса создания сервиса.


Если перейти в управление конкретным контейнером, то у вас появится возможность наблюдать за потреблением ресурсов конкретного контейнера, просмотра логов, а также подключению к интерактивной консоли (да, можно зайти внутрь работающего контейнера в терминальную сессию и выполнять нужные вам команды).

Образы (Images)
Не знаю как вам, а мне кажется очень удобной возможность выделить мышкой сразу 5-6 не используемых образов, а затем удалить их в один клик.


Более того, при клике на каждый конкретный образ можно изменить его тег, получить информацию о размере, дате создания, а также информацию из Dockerfile, такую как, например: CMD, ENTRYPOINT, EXPOSE, VOLUME и переменные окружения внутри контейнера из ENV.


Внедряем и поддерживаем Kubernetes/DevOps
Развертываем и сопровождаем инфраструктуру для бизнес-приложений на базе микросервисов и контейнеров
Узнать больше
Сети (Networks)
Возможностей работы с сетями в Portainer пока не очень много. В списке с сетями отображается лишь скудная информация о типе сети и ее адресации.


Диски (Volumes)
В принципе все, что можно получить из docker volume inspect удобно и наглядно отображается в едином интерфейсе, дополняя остальную функциональность.


Кластеры (Swarm)
Состав кластера, а также количество задействованных узлов и доступных в кластере ресурсов можно узнать в меню Swarm. При выборе каждого конкретного узла кластера можно очень удобно очистить его от виртуальных машин и остановить на нем аллокацию.


Подключения (Endpoints)
Portainer очень удобно использовать вместе со своей Docker Machine, чтобы иметь возможность управлять сразу всеми вашими Docker хостами и кластерами, определить очередное подключение не составит труда в соответствующем меню. Опция поддержки TLS также имеется.


Запуск
В принципе, у Portainer очень достойная документация, но зачем же в нее ходить, если можно получить команду для запуска прямо не покидая страницу обзора?

```
$ docker run -d -p 9000:9000 -v ~/.docker/machine/certs:/certs portainer/portainer -H tcp://192.168.99.100:2376 --tlsverify
```

Данная команда запустит контейнер, содержащий Portainer на 9000 порту вашего Docker Engine. Я запускаю Portainer на своем ноутбуке. Флаг -v смонтирует сертификаты из моей домашней директории внутрь контейнера, чтобы предоставить мне возможность управлять любым Docker хостом или Swarm кластером, настроенным при помощи моей Docker Machine с использованием TLS аутентификации (очень удобно, когда вы администрируете множество хостов). Флаг -H указывает на IP адрес хоста, которым Portainer будет управлять — это нужно исключительно для формирования primary подключения (см последний скриншот).

## 5.6.	Watchtower

Docker – отличный выбор для конвейеров непрерывной интеграции/непрерывного развертывания (CI/CD), поскольку он помогает автоматизировать оба этапа процесса.

Docker-файлы сами по себе обеспечивают способ создания образа вашего приложения, и довольно просто настроить автоматическую сборку контейнеров из исходников на таких сервисах, как Github.

После сборки и размещения в “реджестри образов” его можно загрузить и запустить на любом сервере с Docker.

☸️ Установка Harbor – реджестри образов в Kubernetes / OpenShift с помощью Helm
🐳 Настройка локального реджестри для Docker контейнеров с помощью Podman & Let’s Encrypt SSL
Это замечательно, но это все равно подразумевает выполнение команд на сервере каждый раз, когда вы хотите обновить его.

Если вы хотите, чтобы обновление было действительно автоматическим, то вас может заинтересовать инструмент под названием Watchtower.

Watchtower – это утилита, которая запускается на вашем хосте Docker и периодически проверяет наличие обновлений для контейнеров.

Если она обнаружит новую версию образа из реестра контейнеров, то автоматически уничтожит контейнер и немедленно перезапустит его.

Это очень привлекательная функция, но прежде чем вы приступите к ее установке, важно обсудить ее недостатки.

Выполнение обновлений полностью автоматически означает, что у вас будет меньше контроля над временем и предварительным тестированием развертывания, поскольку всякий раз, когда коммит в вашем репозитории пройдет и вызовет сборку, запущенные контейнеры будут обновляться.

Если вы не являетесь владельцем запускаемого образа, он может обновиться неожиданно, если вы не исключите его из Watchtower.

Если вы не занимаетесь обновлением каждый день, вам может быть лучше использовать графический интерфейс Docker, например Portainer, который позволяет просматривать запущенные контейнеры на ваших серверах и нажимать кнопку для их автоматического обновления.

🐳 Как развернуть стек Portainer внутри кластера Docker Swarm

Это дает вам больше контроля над процессом и может помочь предотвратить неожиданные обновления.

Использование Watchtower
Watchtower упакован как контейнер Docker, поэтому его довольно легко установить – достаточно выполнить всего одну команду, чтобы он заработал:

```
docker run -d 
--name watchtower 
-v /var/run/docker.sock:/var/run/docker.sock 
containrrr/watchtower
```

Система запускает контейнер Watchtower, а также создает бинд, монтирующий docker.sock с хоста, чтобы он мог взаимодействовать с Docker API.

По умолчанию он сканирует все запущенные контейнеры на наличие обновлений каждые 24 часа, что можно изменить с помощью флага –interval.

Вы также можете создать файл docker-compose.yml для запуска Watchtower в качестве сервиса:

```
version: "3"
services:
  watchtower:
    image: containrrr/watchtower
    restart: always
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /root/.docker/config.json:/config.json
    command: --interval 300
```

Вы можете исключить контейнеры из обновлений, но только путем установки метки на сканируемом контейнере.

Вам нужно будет либо установить этот флаг в командной строке во время запуска docker, либо указать его в процессе сборки контейнера с помощью директивы LABEL.

```
docker run -d --label=com.centurylinklabs.watchtower.enable=false nginx

LABEL com.centurylinklabs.watchtower.enable="false"
```

Можно сделать и наоборот, передав флаг –label-enable при запуске Watchtower и установив для контейнеров значение “true”.

Запуск Watchtower в качестве службы Docker Compose
Более эффективный метод, особенно если вам нужно запустить Watchtower только для одного контейнера, – это упаковать его в существующий файл docker-compose.yml и создать scope для сканирования Watchtower.

В этом примере образу NGINX присвоен scope “nginx”, и Watchtower настроен на обновление контейнеров только с этой меткой scope.

```
version: '3'

services:
  nginx:
    image: nginx
    labels:
      - "com.centurylinklabs.watchtower.scope=nginx"

  watchtower:
    image: containrrr/watchtower
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    command: --interval 300 --scope nginx
    labels:
      - "com.centurylinklabs.watchtower.scope=nginx"
```

Использование сторонних реджестри
По умолчанию Watchtower работает только с Docker Hub и любым другим публичным хабом.

Это исключает некоторые сервисы, такие как реджестри контейнеров Github, для получения образов из которого требуется имя пользователя и персональный токен доступа (PAT).

Вы можете добавить конфигурацию для частных реестров, создав файл config.json со следующим содержимым:

```
{
    "auths": {
        "ghcr.io": {
            "auth": "credentials"
        }
    }
}
```

Значение “credentials” должно быть установлено в base64 закодированную строку вашего имени пользователя:пароля или персонального токена доступа в случае Github.

```
echo -n 'username:password' | base64
```

Затем, когда вы запускаете Watchtower, передайте дополнительное монтирование для этого файла config.json.

```
docker run -d 
    -v config.json:/config.json
    -v /var/run/docker.sock:/var/run/docker.sock 
    containrrr/watchtower
```

# 6.	Сеть

## 6.1.	Основы сетей Docker

Сеть Docker в основном используется для установления связи между контейнерами Docker и внешним миром через хост-машину, или вы можете сказать, что это коммуникационный канал, через который все изолированные контейнеры взаимодействуют друг с другом в различных ситуациях для выполнения необходимых действий.
В этом руководстве мы объясним основные концепции работы с сетями Docker на практических примерах Ubuntu.

Все перечисленные ниже команды протестированы с правами root в Ubuntu.

Для управления сетевыми операциями, такими как создание новой сети, подключение контейнера к сети, отключение контейнера от сети, перечисление доступных сетей, удаление сетей и т. д., мы используем следующую команду:

```
# docker network
```

Типы сетевых драйверов Docker
Чтобы получить список всех ваших сетей, запустите:

```
# docker network ls
```

Давайте кратко представим их всех.

## 6.2.  Разновидности сетей Docker

- Bridge network при запуске Docker автоматически создается сеть типа мост по умолчанию. Недавно запущенные контейнеры будут автоматически подключаться к нему. Вы также можете создавать пользовательские настраиваемые мостовые сети. Пользовательские мостовые сети превосходят сетевые мосты по умолчанию.

- Host network : удаляет сетевую изоляцию между контейнером и хостом Docker и напрямую использует сеть хоста. Если вы запускаете контейнер, который привязывается к порту 80, и вы используете хост-сеть, приложение контейнера доступно через порт 80 по IP-адресу хоста. Означает, что вы не сможете запускать несколько веб-контейнеров на одном хосте, на одном и том же порту, так как порт теперь является общим для всех контейнеров в сети хоста.

- None network : в сети такого типа контейнеры не подключены ни к одной сети и не имеют доступа к внешней сети или другим контейнерам. Итак, эта сеть используется, когда вы хотите полностью отключить сетевой стек в контейнере.

- Overlay network : Создает внутреннюю частную сеть, которая охватывает все узлы, участвующие в кластере swarm. Таким образом, оверлейные сети облегчают обмен данными между сервисом Docker Swarm и автономным контейнером или между двумя автономными контейнерами на разных демонах Docker.

- Macvlan network : Некоторые приложения, особенно устаревшие приложения, отслеживающие сетевой трафик, ожидают прямого подключения к физической сети. В такой ситуации вы можете использовать сетевой драйвер Macvlan для назначения MAC-адреса виртуальному сетевому интерфейсу каждого контейнера, что делает его физическим сетевым интерфейсом, напрямую подключенным к физической сети.


## 6.3.	Основные команды сетей Docker



## 6.4.	Работа с сетями контейнера

практические упражнения для сетей Bridge и Host.

Сеть типа bridge

Я буду использовать два Alpine-контейнера для объяснения этого типа сети.

Теперь я собираюсь запустить два контейнера Alpine, а именно C1 и C2, используя команды:

```
# docker run -it -d --name c1 alpine ash
# docker run -it -d --name c2 alpine ash
```

Далее, давайте выясним IP-адрес этих запущенных контейнеров. Для этого запустите:

```
# docker exec -it c1 sh –c “ip a”
# docker exec -it c2 sh –c “ip a”
```

Как видите, IP-адрес контейнера C1 – 172.17.0.2, а IP-адрес C2 – 172.17.0.3.

Теперь давайте продолжим и попытаемся проверить связь друг с другом, чтобы убедиться, что они могут общаться.

Сначала подключитесь к работающему контейнеру C1 и попробуйте проверить связь с контейнером C2:

```
# docker attach c1
# Ping –c 2 172.17.0.3
```

Как вы можете видеть на скриншотах, показанных выше, происходит конекшен между контейнерами в одной сети.

Мы также можем проверить это, проверив сеть типа мост с помощью команды:

```
# docker network inspect bridge
```

Приведенная выше команда отобразит всю информацию о сети, такую как тип сети, подсеть, шлюз, имя контейнера и IP-адрес и т. д.

1.1 Создание пользовательской Bridge сети
Как я уже говорил, при запуске Docker автоматически создается сеть bridge по умолчанию.

Все вновь запущенные контейнеры будут автоматически подключаться к ней.

Однако вы также можете создавать пользовательские мостовые сети.

Чтобы создать новый сетевой драйвер, просто запустите:

```
# docker network create my_net
```

или

```
# docker network create --driver bridge dhruv_net
```

Обе команды будут выполнять одинаковую работу.
Если вы не укажете имя драйвера, сеть будет создана в сетевом драйвере по умолчанию, т.е. bridge.

В пользовательских сетях, таких как dhruv_net, контейнеры могут не только связываться по IP-адресу, но также могут преобразовывать имя контейнера в IP-адрес.
Эта возможность называется автоматическим обнаружением службы.

Чтобы убедиться, что контейнеры могут взаимодействовать друг с другом, давайте запустим три alpine контейнера, а именно A1, A2 и A3 в сети dhruv_net, которую мы создали ранее.

```
# docker run -it -d --name A1 --network dhruv_net alpine ash
# docker run -it -d --name A2 --network dhruv_net alpine ash
# docker run -it -d --name A3 --network dhruv_net alpine ash
```

Теперь попробуйте подключиться к любому из контейнеров и пропинговать два других, используя имя контейнера.

Из приведенных выше снимков экрана доказано, что контейнеры могут общаться друг с другом.
2. Хостовая Сеть
Мы запускаем контейнер, который привязывается к порту 80, используя хост-сеть, приложение контейнера доступно через порт 80 по IP-адресу хоста.



Сеть типа Host нужна только тогда, когда вы запускаете программы с очень специфической сетью.
Приложения, работающие внутри контейнера Docker, выглядят так, как будто они работают на самом хосте с точки зрения сети.
Это позволяет контейнеру получить больший доступ к сети, чем он может получить.

Здесь мы использовали команду netstat -ntlp для отображения порта прослушивания на сервере.
Чтобы узнать, какая служба прослушивает определенный порт, это руководство.
Как перечислить службы в Linux
Как узнать, какой номер порта используется процессом в Linux?

Мы рассмотрели только основы сетевых концепций Docker.
Для более подробной информации, я предлагаю вам заглянуть в руководство по работе с сетями Docker, которое прилагается ниже.
https://docs.docker.com/v17.09/engine/userguide/networking/#exposing-and-publishing-ports

# 7.	Хранение данных

## 7.1.	Основы хранения данных в Docker

Важная характеристика Docker-контейнеров — эфемерность. В любой момент контейнер может рестартовать: завершиться и вновь запуститься из образа. При этом все накопленные в нём данные будут потеряны. Но как в таком случае запускать в Docker приложения, которые должны сохранять информацию о своём состоянии? Для этого есть несколько инструментов.

В этой статье рассмотрим 

- docker volumes, 
- bind mount и 
- tmpfs, 

дадим советы по их использованию, проведём небольшую практику.

**Особенности работы контейнеров**

Прежде чем перейти к способам хранения данных, вспомним устройство контейнеров. Это поможет лучше понять основную тему.

Контейнер создаётся из образа, в котором есть всё для начала его работы. Но там не хранится и тем более не изменяется ничего важного. В любой момент приложение в контейнере может быть завершено, а контейнер уничтожен, и это нормально. Контейнер отработал — выкидываем его и собираем новый. Если пользователь загрузил в приложение картинку, то при замене контейнера она удалится.




На схеме показано устройство контейнера, запущенного из образа Ubuntu 15.04. Контейнер состоит из пяти слоёв: четыре из них принадлежат образу, и лишь один — самому контейнеру. Слои образа доступны только для чтения, слой контейнера — для чтения и для записи. Если при работе приложения какие-то данные будут изменяться, они попадут в слой контейнера. Но при уничтожении контейнера слой будет безвозвратно потерян, и все данные вместе с ним.

В идеальном мире Docker используют только для запуска stateless-приложений, которые не читают и не сохраняют данные о своём состоянии и готовы в любой момент завершиться. Однако в реальности большинство программ относятся к категории stateful, то есть требуют сохранения данных между перезапусками.

Поэтому нужны способы сделать так, чтобы важные изменяемые данные не зависели от эфемерности контейнеров и, как бонус, были доступными сразу из нескольких мест.

В Docker есть несколько способов хранения данных. Наиболее распространенные:

- тома хранения данных (docker volumes),
- монтирование каталогов с хоста (bind mount).

Особые типы хранения:

- именованные каналы (named pipes, только в Windows),
- монтирование tmpfs (только в Linux).




На схеме показаны самые популярные типы хранения данных для Linux: в памяти (tmpfs), в файловой системе хоста (bind mount), в томе Docker (docker volumes). Разберём каждый вариант.

**Тома (docker volumes)**

Тома — рекомендуемый разработчиками Docker способ хранения данных. В Linux тома находятся по умолчанию в /var/lib/docker/volumes/. Другие программы не должны получать к ним доступ напрямую, только через контейнер.

Тома создаются и управляются средствами Docker: командой docker volume create, через указание тома при создании контейнера в Dockerfile или docker-compose.yml.

В контейнере том видно как обычный каталог, который мы определяем в Dockerfile. Тома могут быть с именами или без — безымянным томам Docker сам присвоит имя.

Один том может быть примонтирован одновременно в несколько контейнеров. Когда никто не использует том, он не удаляется, а продолжает существовать. Команда для удаления томов: docker volume prune.

Можно выбрать специальный драйвер для тома и хранить данные не на хосте, а на удалённом сервере или в облаке.

Для чего стоит использовать тома в Docker:

- шаринг данных между несколькими запущенными контейнерами,
- решение проблемы привязки к ОС хоста,
- удалённое хранение данных,
- бэкап или миграция данных на другой хост с Docker (для этого надо остановить все контейнеры и скопировать содержимое из каталога тома в нужное место).

**Монтирование каталога с хоста (bind mount)**

Это более простая концепция: файл или каталог с хоста просто монтируется в контейнер.

Используется, когда нужно пробросить в контейнер конфигурационные файлы с хоста. Например, именно так в контейнерах реализуется DNS: с хоста монтируется файл /etc/resolv.conf.

Другое очевидное применение — в разработке. Код находится на хосте (вашем ноутбуке), но исполняется в контейнере. Вы меняете код и сразу видите результат. Это возможно, так как процессы хоста и контейнера одновременно имеют доступ к одним и тем же данным.

Особенности bind mount:

Запись в примонтированный каталог могут вести программы как в контейнере, так и на хосте. Это значит, есть риск случайно затереть данные, не понимая, что с ними работает контейнер.
Лучше не использовать в продакшене. Для продакшена убедитесь, что код копируется в контейнер, а не монтируется с хоста.
Для успешного монтирования указывайте полный путь к файлу или каталогу на хосте.
Если приложение в контейнере запущено от root, а совместно используется каталог с ограниченными правами, то в какой-то момент может возникнуть проблема с правами на файлы и невозможность что-то удалить без использования sudo.

Когда использовать тома, а когда монтирование с хоста

**Volume	Bind mount**

Просто расшарить данные между контейнерами.	Пробросить конфигурацию с хоста в контейнер.
У хоста нет нужной структуры каталогов.	Расшарить исходники и/или уже собранные приложения.
Данные лучше хранить не локально (а в облаке, например).	Есть стабильная структура каталогов и файлов, которую нужно расшарить между контейнерами.

**Монтирование tmpfs**

Tmpfs — временное файловое хранилище. Это некая специально отведённая область в оперативной памяти компьютера. Из определения выходит, что tmpfs — не лучшее хранилище для важных данных. Так оно и есть: при остановке или перезапуске контейнера сохранённые в tmpfs данные будут навсегда потеряны.

На самом деле tmpfs нужно не для сохранения данных, а для безопасности, полученные в ходе работы приложения чувствительные данные безвозвратно исчезнут после завершения работы контейнера. Бонусом использования будет высокая скорость доступа к информации.

Например, приложение в контейнере тормозит из-за того, что в ходе работы активно идут операции чтения-записи, а диски на хосте не очень быстрые. Если вы не уверены, в какой каталог идёт эта нагрузка, можно применить к запущенному контейнеру команду docker diff. И вот этот каталог смонтировать как tmpfs, таким образом перенеся ввод-вывод с диска в оперативную память.

Такое хранилище может одновременно работать только с одним контейнером и доступно только в Linux.


**Общие советы по использованию томов**

**Монтирование в непустые директории**

Если вы монтируете пустой том в каталог контейнера, где уже есть файлы, то эти файлы не удалятся, а будут скопированы в том. Этим можно пользоваться, когда нужно скопировать данные из одного контейнера в другой.

Если вы монтируете непустой том или каталог с хоста в контейнер, где уже есть файлы, то эти файлы тоже не удалятся, а просто будут скрыты. Видно будет только то, что есть в томе или каталоге на хосте. Похоже на простое монтирование в Linux.

**Монтирование служебных файлов**

С хоста можно монтировать любые файлы, в том числе служебные. Например, сокет docker. В результате получится docker-in-docker: один контейнер запустится внутри другого. UPD: (*это не совсем так. mwizard в комментариях пояснил, что в таком случае родительский docker запустит sibling-контейнер). Выглядит как бред, но в некоторых случаях бывает оправдано. Например, при настройке CI/CD.

**Монтирование /var/lib/docker**

Разработчики Docker говорят, что не стоит монтировать с хоста каталог /var/lib/docker, так как могут возникнуть проблемы. Однако есть некоторые программы, для запуска которых это необходимо.

**Практика: создадим тестовый том**

Ключ командной строки для Docker при работе с томами.

Для volume или bind mount:

```
--volume | -v
```

Для tmpfs:

```
--tmpfs
```

Команды для управления томами в интерфейсе CLI Docker:

```
$ docker volume

Commands:
  create   Create a volume (Создать том)
  inspect  Display detailed information on one or more
        volumes (Отобразить детальную информацию)
  ls    List volumes (Вывести список томов)
  prune Remove all unused volumes (Удалить все неиспользуемые тома)
  rm    Remove one or more volumes (Удалить один или несколько томов)
```

Создадим тестовый том:

```
$ docker volume create slurm-storage
slurm-storage
```

Вот он появился в списке:

```
$ docker volume ls
DRIVER  VOLUME NAME
local   slurm-storage
```

Команда inspect выдаст примерно такой список информации в json:

```
$ docker inspect slurm-storage
[
    {
        "CreatedAt": "2020-12-14T15:00:37Z",
        "Driver": "local",
        "Labels": {},
        "Mountpoint": "/var/lib/docker/volumes/slurm-storage/_data",
        "Name": "slurm-storage",
        "Options": {},
        "Scope": "local"
    }
]
```

Попробуем использовать созданный том, запустим с ним контейнер:

```
$ docker run --rm -v slurm-storage:/data -it ubuntu:20.10 /bin/bash
# echo $RANDOM > /data/file
# cat /data/file
13279
# exit
```

После самоуничтожения контейнера запустим другой и подключим к нему тот же том. Проверяем, что в нашем файле:

```
$ docker run --rm -v slurm-storage:/data -it centos:8 /bin/bash -c "cat /data/file"
13279
```

То же самое, отлично.

Теперь примонтируем каталог с хоста:

```
$ docker run -v /srv:/host/srv --name slurm --rm -it ubuntu:20.10 /bin/bash
```

Docker не любит относительные пути, лучше указывайте абсолютные!

Теперь попробуем совместить оба типа томов сразу:

```
$ docker run -v /srv:/host/srv -v slurm-storage:/data --name slurm --rm -it ubuntu:20.10 /bin/bash
```

Отлично! А если нам нужно передать ровно те же тома другому контейнеру?

```
$ docker run --volumes-from slurm --name backup --rm -it centos:8 /bin/bash
```

Вы можете заметить некий лаг в обновлении данных между контейнерами, это зависит от используемого Docker драйвера файловой системы.

Создавать том заранее необязательно, всё сработает в момент запуска docker run:

```
$ docker run -v newslurm:/newdata -v /srv:/host/srv -v slurm-storage:/data --name slurm --rm -it ubuntu:20.10 /bin/bash
```

Посмотрим теперь на список томов:

```
$ docker volume ls
DRIVER  VOLUME NAME
local   slurm-storage
local   newslurm
```

Ещё немного усложним команду запуска, создадим анонимный том:

```
$ docker run -v /anonymous -v newslurm:/newdata -v /srv:/host/srv -v slurm-storage:/data --name slurm --rm -it ubuntu:20.10 /bin/bash
```

Такой том самоуничтожится после выхода из контейнера, так как мы указали ключ –rm.

Если этого не сделать, давайте проверим что будет:

```
$ docker run -v /anonymous -v newslurm:/newdata -v /srv:/host/srv -v slurm-storage:/data --name slurm -it ubuntu:20.10 /bin/bash
$ docker volume ls
DRIVER  VOLUME NAME
local     04c490b16184bf71015f7714b423a517ce9599e9360af07421ceb54ab96bd333
local   newslurm
local   slurm-storage
```

**Хозяйке на заметку**: тома (как образы и контейнеры) ограничены значением настройки dm.basesize, которая устанавливается на уровне настроек демона Docker. Как правило, что-то около 10Gb. Это значение можно изменить вручную, но потребуется перезапуск демона Docker.

При запуске демона с ключом это выглядит так:

```
$ sudo dockerd --storage-opt dm.basesize=40G
```

Однажды увеличив значение, его уже нельзя просто так уменьшить. При запуске Docker выдаст ошибку.

Если вам нужно вручную очистить содержимое всех томов, придётся удалять каталог, предварительно остановив демон:

```
$ sudo service docker stop
$ sudo rm -rf /var/lib/docker
```

Если вам интересно узнать подробнее о работе с данными в Docker и других возможностях технологии, приглашаем на двухдневный онлайн-интенсив в феврале. Будет много практики.
Автор статьи: Александр Швалов, практикующий инженер Southbridge, Certified Kubernetes Administrator, автор и разработчик курсов Слёрм.

## 7.2.	Основные команды разделов



## 7.3.	Bind Mount



## 7.4.	Работа с разделами



# 8.	Docker-compose

## 8.1.	Что такое docker-compose

**Docker Compose** — это инструментальное средство, входящее в состав Docker. Оно предназначено для решения задач, связанных с развёртыванием проектов.

Изучая основы Docker, вы могли столкнуться с созданием простейших приложений, работающих автономно, не зависящих, например, от внешних источников данных или от неких сервисов. На практике же подобные приложения — редкость. Реальные проекты обычно включают в себя целый набор совместно работающих приложений.

Как узнать, нужно ли вам, при развёртывании некоего проекта, воспользоваться Docker Compose? На самом деле — очень просто. Если для обеспечения функционирования этого проекта используется несколько сервисов, то Docker Compose может вам пригодиться. Например, в ситуации, когда создают веб-сайт, которому, для выполнения аутентификации пользователей, нужно подключиться к базе данных. Подобный проект может состоять из двух сервисов — того, что обеспечивает работу сайта, и того, который отвечает за поддержку базы данных.

Технология Docker Compose, если описывать её упрощённо, позволяет, с помощью одной команды, запускать множество сервисов.

**Разница между Docker и Docker Compose**

Docker применяется для управления отдельными контейнерами (сервисами), из которых состоит приложение.

Docker Compose используется для одновременного управления несколькими контейнерами, входящими в состав приложения. Этот инструмент предлагает те же возможности, что и Docker, но позволяет работать с более сложными приложениями.

Docker (отдельный контейнер) и Docker Compose (несколько контейнеров)

Типичный сценарий использования Docker Compose

Docker Compose — это, в умелых руках, весьма мощный инструмент, позволяющий очень быстро развёртывать приложения, отличающиеся сложной архитектурой. Сейчас мы рассмотрим пример практического использования Docker Compose, разбор которого позволит вам оценить те преимущества, которые даст вам использование Docker Compose.

Представьте себе, что вы являетесь разработчиком некоего веб-проекта. В этот проект входит два веб-сайта. Первый позволяет людям, занимающимся бизнесом, создавать, всего в несколько щелчков мышью, интернет-магазины. Второй нацелен на поддержку клиентов. Эти два сайта взаимодействуют с одной и той же базой данных.

Ваш проект становится всё популярнее, и оказывается, что мощности сервера, на котором он работает, уже недостаточно. В результате вы решаете перевести весь проект на другую машину.

К сожалению, нечто вроде Docker Compose вы не использовали. Поэтому вам придётся переносить и перенастраивать сервисы по одному, надеясь на то, что вы, в процессе этой работы, ничего не забудете.

Если же вы используете Docker Compose, то перенос вашего проекта на новый сервер — это вопрос, который решается выполнением нескольких команд. Для того чтобы завершить перенос проекта на новое место, вам нужно лишь выполнить кое-какие настройки и загрузить на новый сервер резервную копию базы данных.

**Разработка клиент-серверного приложения с использованием Docker Compose**

Теперь, когда вы знаете о том, для чего мы собираемся использовать Docker Compose, пришло время создать ваше первое клиент-серверное приложение с использованием этого инструмента. А именно, речь идёт о разработке небольшого веб-сайта (сервера) на Python, который умеет выдавать файл с фрагментом текста. Этот файл у сервера запрашивает программа (клиент), тоже написанная на Python. После получения файла с сервера программа выводит текст, хранящийся в нём, на экран.

Обратите внимание на то, что мы рассчитываем на то, что вы владеете основами Docker, и на то, что у вас уже установлена платформа Docker.

Приступим к работе над проектом.

1. Создание проекта

Для того чтобы построить ваше первое клиент-серверное приложение, предлагаю начать с создания папки проекта. Она должна содержать следующие файлы и папки:

Файл docker-compose.yml. Это файл Docker Compose, который будет содержать инструкции, необходимые для запуска и настройки сервисов.
Папка server. Она будет содержать файлы, необходимые для обеспечения работы сервера.
Папка client. Здесь будут находиться файлы клиентского приложения.

В результате содержимое главной папки вашего проекта должно выглядеть так:

```
.
├── client/
├── docker-compose.yml
└── server/
2 directories, 1 file
```

2. Создание сервера

Тут мы, в процессе создания сервера, затронем некоторые базовые вещи, касающиеся Docker.

2a. Создание файлов

Перейдите в папку server и создайте в ней следующие файлы:

- Файл server.py. В нём будет находиться код сервера.
- Файл index.html. В этом файле будет находиться фрагмент текста, который должно вывести клиентское приложение.
- Файл Dockerfile. Это — файл Docker, который будет содержать инструкции, необходимые для создания окружения сервера.

Вот как должно выглядеть содержимое вашей папки server/:

```
.
├── Dockerfile
├── index.html
└── server.py
0 directories, 3 files
```

2b. Редактирование Python-файла.

Добавим в файл server.py следующий код:

```
#!/usr/bin/env python3

# Импорт системных библиотек python.
# Эти библиотеки будут использоваться для создания веб-сервера.
# Вам не нужно устанавливать что-то особенное, эти библиотеки устанавливаются вместе с Python.

import http.server
import socketserver

# Эта переменная нужна для обработки запросов клиента к серверу.

handler = http.server.SimpleHTTPRequestHandler

# Тут мы указываем, что сервер мы хотим запустить на порте 1234. 
# Постарайтесь запомнить эти сведения, так как они нам очень пригодятся в дальнейшем, при работе с docker-compose.

with socketserver.TCPServer(("", 1234), handler) as httpd:

    # Благодаря этой команде сервер будет выполняться постоянно, ожидая запросов от клиента.

   httpd.serve_forever()
```

Этот код позволяет создать простой веб-сервер. Он будет отдавать клиентам файл index.html, содержимое которого позже будет выводиться на веб-странице.

2c. Редактирование HTML-файла

В файл index.html добавим следующий текст:

```
Docker-Compose is magic!
```

Этот текст будет передаваться клиенту.

2d. Редактирование файла Dockerfile

Сейчас мы создадим простой файл Dockerfile, который будет отвечать за организацию среды выполнения для Python-сервера. В качестве основы создаваемого образа воспользуемся официальным образом, предназначенным для выполнения программ, написанных на Python. Вот содержимое Dockerfile:

```
# На всякий случай напоминаю, что Dockerfile всегда должен начинаться с импорта базового образа.
# Для этого используется ключевое слово 'FROM'.
# Здесь нам нужно импортировать образ python (с DockerHub).
# В результате мы, в качестве имени образа, указываем 'python', а в качестве версии - 'latest'.

FROM python:latest

# Для того чтобы запустить в контейнере код, написанный на Python, нам нужно импортировать файлы 'server.py' и 'index.html'.
# Для того чтобы это сделать, мы используем ключевое слово 'ADD'.
# Первый параметр, 'server.py', представляет собой имя файла, хранящегося на компьютере.
# Второй параметр, '/server/', это путь, по которому нужно разместить указанный файл в образе.
# Здесь мы помещаем файл в папку образа '/server/'.

ADD server.py /server/
ADD index.html /server/

# Здесь мы воспользуемся командой 'WORKDIR', возможно, новой для вас.
# Она позволяет изменить рабочую директорию образа.
# В качестве такой директории, в которой будут выполняться все команды, мы устанавливаем '/server/'.

WORKDIR /server/
```

Теперь займёмся работой над клиентом.

3. Создание клиента

Создавая клиентскую часть нашего проекта, мы попутно вспомним некоторые основы Docker.

3a. Создание файлов

Перейдите в папку вашего проекта client и создайте в ней следующие файлы:

- Файл client.py. Тут будет находиться код клиента.
- Файл Dockerfile. Этот файл играет ту же роль, что и аналогичный файл в папке сервера. А именно, он содержит инструкцию, описывающую создание среды для выполнения клиентского кода.

В результате ваша папка client/ на данном этапе работы должна выглядеть так:

```
.
├── client.py
└── Dockerfile
0 directories, 2 files
```

3b. Редактирование Python-файла

Добавим в файл client.py следующий код:

```
#!/usr/bin/env python3

# Импортируем системную библиотеку Python.
# Она используется для загрузки файла 'index.html' с сервера.
# Ничего особенного устанавливать не нужно, эта библиотека устанавливается вместе с Python.

import urllib.request

# Эта переменная содержит запрос к 'http://localhost:1234/'.
# Возможно, сейчас вы задаётесь вопросом о том, что такое 'http://localhost:1234'.
# localhost указывает на то, что программа работает с локальным сервером.
# 1234 - это номер порта, который вам предлагалось запомнить при настройке серверного кода.

fp = urllib.request.urlopen("http://localhost:1234/")

# 'encodedContent' соответствует закодированному ответу сервера ('index.html').
# 'decodedContent' соответствует раскодированному ответу сервера (тут будет то, что мы хотим вывести на экран).

encodedContent = fp.read()
decodedContent = encodedContent.decode("utf8")

# Выводим содержимое файла, полученного с сервера ('index.html').

print(decodedContent)

# Закрываем соединение с сервером.

fp.close()
```

Благодаря этому коду клиентское приложение может загрузить данные с сервера и вывести их на экран.

3c. Редактирование файла Dockerfile

Как и в случае с сервером, мы создаём для клиента простой Dockerfile, ответственный за формирование среды, в которой будет работать клиентское Python-приложение. Вот код клиентского Dockerfile:

```
# То же самое, что и в серверном Dockerfile.

FROM python:latest

# Импортируем 'client.py' в папку '/client/'.

ADD client.py /client/

# Устанавливаем в качестве рабочей директории '/client/'.

WORKDIR /client/
```

4. Docker Compose

Как вы могли заметить, мы создали два разных проекта: сервер и клиент. У каждого из них имеется собственный файл Dockerfile. До сих пор всё происходящее не выходит за рамки основ работы с Docker. Теперь же мы приступаем к работе с Docker Compose. Для этого обратимся к файлу docker-compose.yml, расположенному в корневой папке проекта.

Обратите внимание на то, что тут мы не стремимся рассмотреть абсолютно все команды, которые можно использовать в docker-compose.yml. Наша главная цель — разобрать практический пример, дающий вам базовые знания по Docker Compose.

Вот код, который нужно поместить в файл docker-compose.yml:

```
# Файл docker-compose должен начинаться с тега версии.
# Мы используем "3" так как это - самая свежая версия на момент написания этого кода.

version: "3"

# Следует учитывать, что docker-composes работает с сервисами.
# 1 сервис = 1 контейнер.
# Сервисом может быть клиент, сервер, сервер баз данных...
# Раздел, в котором будут описаны сервисы, начинается с 'services'.

services:

  # Как уже было сказано, мы собираемся создать клиентское и серверное приложения.
  # Это означает, что нам нужно два сервиса.
  # Первый сервис (контейнер): сервер.
  # Назвать его можно так, как нужно разработчику.
  # Понятное название сервиса помогает определить его роль.
  # Здесь мы, для именования соответствующего сервиса, используем ключевое слово 'server'.

  server:
 
    # Ключевое слово "build" позволяет задать
    # путь к файлу Dockerfile, который нужно использовать для создания образа,
    # который позволит запустить сервис.
    # Здесь 'server/' соответствует пути к папке сервера,
    # которая содержит соответствующий Dockerfile.

    build: server/

    # Команда, которую нужно запустить после создания образа.
    # Следующая команда означает запуск "python ./server.py".

    command: python ./server.py

    # Вспомните о том, что в качестве порта в 'server/server.py' указан порт 1234.
    # Если мы хотим обратиться к серверу с нашего компьютера (находясь за пределами контейнера),
    # мы должны организовать перенаправление этого порта на порт компьютера.
    # Сделать это нам поможет ключевое слово 'ports'.
    # При его использовании применяется следующая конструкция: [порт компьютера]:[порт контейнера]
    # В нашем случае нужно использовать порт компьютера 1234 и организовать его связь с портом
    # 1234 контейнера (так как именно на этот порт сервер 
    # ожидает поступления запросов).

    ports:
      - 1234:1234

  # Второй сервис (контейнер): клиент.
  # Этот сервис назван 'client'.

  client:
    # Здесь 'client/ соответствует пути к папке, которая содержит
    # файл Dockerfile для клиентской части системы.

    build: client/

    # Команда, которую нужно запустить после создания образа.
    # Следующая команда означает запуск "python ./client.py".
 
    command: python ./client.py

    # Ключевое слово 'network_mode' используется для описания типа сети.
    # Тут мы указываем то, что контейнер может обращаться к 'localhost' компьютера.

    network_mode: host

    # Ключевое слово 'depends_on' позволяет указывать, должен ли сервис,
    # прежде чем запуститься, ждать, когда будут готовы к работе другие сервисы.
    # Нам нужно, чтобы сервис 'client' дождался бы готовности к работе сервиса 'server'.
 
    depends_on:
      - server
```

5. Сборка проекта

После того, как в docker-compose.yml внесены все необходимые инструкции, проект нужно собрать. Этот шаг нашей работы напоминает использование команды docker build, но соответствующая команда имеет отношение к нескольким сервисам:

```
$ docker-compose build
```

6. Запуск проекта

Теперь, когда проект собран, пришло время его запустить. Этот шаг нашей работы соответствует шагу, на котором, при работе с отдельными контейнерами, выполняется команда docker run:

```
$ docker-compose up
```

После выполнения этой команды в терминале должен появиться текст, загруженный клиентом с сервера: Docker-Compose is magic!.

Как уже было сказано, сервер использует порт компьютера 1234 для обслуживания запросов клиента. Поэтому, если перейти в браузере по адресу http://localhost:1234/, в нём будет отображена страница с текстом Docker-Compose is magic!.

**Полезные команды**

Рассмотрим некоторые команды, которые могут вам пригодиться при работе с Docker Compose.

Эта команда позволяет останавливать и удалять контейнеры и другие ресурсы, созданные командой docker-compose up:

```
$ docker-compose down
```

Эта команда выводит журналы сервисов:

```
$ docker-compose logs -f [service name]
```

Например, в нашем проекте её можно использовать в таком виде: $ docker-compose logs -f [service name].

С помощью такой команды можно вывести список контейнеров:

```
$ docker-compose ps
```

Данная команда позволяет выполнить команду в выполняющемся контейнере:

```
$ docker-compose exec [service name] [command]
```

Например, она может выглядеть так: docker-compose exec server ls.

Такая команда позволяет вывести список образов:

```
$ docker-compose images
```

## 8.2.	Состав docker-compose



## 8.3.	Основные команды Docker-compose



## 8.4.	Файл конфигурации Docker-compose



## 8.5.	Разделы и сети в compose



## 8.6.	Установка и управление несколькими контейнерами на выделенном узле




# 9.	Docker Swarm

## 9.1.	Основы Docker Swarm

Данная статья посвящена настройке и работе с Docker Swarm.

Swarm это стандартный оркестратор для docker контейнеров, доступный из «коробки», если у вас установлен сам docker.

Что нам потребуется для освоения:

Иметь опыт работы с docker и docker compose.

Настроенный docker registry. Swarm не очень любит работать с локальными образами.

Несколько виртуальных машин для создания кластера, хотя по факту кластер может состоять из одной виртуалки, но так будет нагляднее.

Термины
Для того чтобы пользоваться swarm надо запомнить несколько типов сущностей:

**Node** - это наши виртуальные машины, на которых установлен docker. Есть manager и workers ноды. Manager нода управляет workers нодами. Она отвечает за создание/обновление/удаление сервисов на workers, а также за их масштабирование и поддержку в требуемом состоянии. Workers ноды используются только для выполнения поставленных задач и не могут управлять кластером.

**Stack** - это набор сервисов, которые логически связаны между собой. По сути это набор сервисов, которые мы описываем в обычном compose файле. Части stack (services) могут располагаться как на одной ноде, так и на разных.

**Service** - это как раз то, из чего состоит stack. Service является описанием того, какие контейнеры будут создаваться. Если вы пользовались docker-compose.yaml, то уже знакомы с этой сущностью. Кроме стандартных полей docker в режиме swarm поддерживает ряд дополнительных, большинство из которых находятся внутри секции deploy.

**Task** - это непосредственно созданный контейнер, который docker создал на основе той информации, которую мы указали при описании service. Swarm будет следить за состоянием контейнера и при необходимости его перезапускать или перемещать на другую ноду.


**Создание кластера**
Для того чтобы кластер корректно работал, необходимо открыть следующие порты на виртуальных машинах. Для manager node:

```
firewall-cmd --add-port=2376/tcp --permanent;
firewall-cmd --add-port=2377/tcp --permanent;
firewall-cmd --add-port=7946/tcp --permanent;
firewall-cmd --add-port=7946/udp --permanent;
firewall-cmd --add-port=4789/udp --permanent;
firewall-cmd --reload;

systemctl restart docker;
```

Для worker node

```
firewall-cmd --add-port=2376/tcp --permanent;
firewall-cmd --add-port=7946/tcp --permanent;
firewall-cmd --add-port=7946/udp --permanent;
firewall-cmd --add-port=4789/udp --permanent;
firewall-cmd --reload;

systemctl restart docker;
```

Затем заходим на виртуальную машину, которая будет у нас manager node. И выполняем следующую команду:

```
docker swarm init
```

Если все успешно, то в ответ вы получите следующую команду:

```
docker swarm join --token SWMTKN-1-54k2k418tw2j0juwm3inq6crp4ow6xogswihcc5azg7oq5qo7e-a3rfeyfwo7d93heq0y5vhyzod 172.31.245.104:2377
```

Ее будет необходимо выполнить на всех worker node, чтобы присоединить их в только что созданный кластер.

Если все прошло успешно, выполнив следующую команду на manager ноде в консоли, вы увидите что-то подобное:

```
docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
vj7kp5847rh5mbbqn97ghyb72 *   dev-2      Ready     Active         Leader           20.10.14
zxo15m9wqdjd9f8pvg4gg6gwi     stage      Ready     Active                          20.10.9
```

У меня виртуалка с hostname dev-2 является manager, а stage является worker нодой.

В принципе мы готовы к тому, чтобы запускать sevices и stacks на нашей worker node.

Если мы хотим убрать ноду из кластера, необходимо зайти на виртуалку, которая является ею, и выполнить команду:

```
docker swarm leave
```

Если затем зайти на manager ноду и выполнить docker node ls, вы заметите, что статус у нее поменялся c Ready на Down (это может занять некоторое время). Swarm больше не будет использовать данную ноду для размещения контейнеров, и вы можете спокойно заняться техническими работами, не боясь нанести вред работающим контейнерам. Для того чтобы окончательно удалить ноду, надо выполнить (на manager node):

```
docker node rm stage
```

**Стэк и Сервис**

Для создания нашего стэка я возьму в качестве примера compose файл для node js web server, который прослушивает порт 4003:

```
# docker-compose.stage.yaml

version: "3.9"

services:
  back:
    image: docker-registry.ru:5000/ptm:stage
    ports:
      - "4003:4003"
    environment:
      TZ: "Europe/Moscow"
    extra_hosts:
      - host.docker.internal:host-gateway
    command: make server_start
    volumes:
      - /p/ptm/config/config.yaml:/p/ptm/config/config.yaml
      - /p/ptm/stat/web:/p/ptm/stat/web
```

В начале необходимо достать image из registry и только затем задеплоить в наш кластер:

```
docker pull docker-registry.ru:5000/ptm:stage;
docker stack deploy --with-registry-auth -c ./docker-compose.stage.yaml stage;
```

Все эти команды надо выполнять на manager node. Опция --with-registry-auth позволяет передать авторизационные данные на worker ноды, для того чтобы использовался один и тот же образ из регистра. stage это имя нашего стэка.

Посмотреть список стэков можно с помощью:

```
docker stack ls
NAME        SERVICES   ORCHESTRATOR
stage       1          Swarm
```

Список сервисов внутри стэка:

```
docker stack services stage
ID             NAME         MODE         REPLICAS   IMAGE                               PORTS
qq8spne5gik7   stage_back   replicated   1/1        docker-registry.ru:5000/ptm:stage   *:4003->4003/tcp
```

Подробная информация о сервисе:

```
docker service ps --no-trunc stage_back
ID                          NAME               IMAGE                                                                                                       NODE      DESIRED STATE   CURRENT STATE           ERROR     PORTS
qok4z2rbl9v8238phy3lxgnw9   stage_back.1       docker-registry.ru:5000/ptm:stage@sha256:8f5dca792c4e8f1bf94cb915505839a73eb50f38540e7d6d537a305594e51cae   stage     Running         Running 1 minute ago              
khctf0lqbk5l9de81ifwuzwer    \_ stage_back.1   docker-registry.ru:5000/ptm:stage@sha256:b4ff5e2fbb0daba7985122098d45a5873f1991fd8734a3a89f5affa5daf96e43   stage     Shutdown        Shutdown 40 hours ago             
```

Тут видно, что для этого сервиса уже запускался контейнер (40 часов назад).

Для того чтобы увидеть более подробную информацию по сервису в виде JSON:

```
 docker service inspect stage_back
 
 [
    {
        "ID": "qq8spne5gik7lqisqwo6u7ybr",
        "Version": {
            "Index": 24526
        },
        "CreatedAt": "2022-04-06T18:37:15.002080284Z",
        "UpdatedAt": "2022-04-06T18:51:09.9827704Z",
        .......
        "Endpoint": {
            "Spec": {
                "Mode": "vip",
                "Ports": [
                    {
                        "Protocol": "tcp",
                        "TargetPort": 4003,
                        "PublishedPort": 4003,
                        "PublishMode": "ingress"
                    }
                ]
            },
            "Ports": [
                {
                    "Protocol": "tcp",
                    "TargetPort": 4003,
                    "PublishedPort": 4003,
                    "PublishMode": "ingress"
                }
            ],
            "VirtualIPs": [
                {
                    "NetworkID": "obmqmpiujnamih7k76q87c058",
                    "Addr": "10.0.0.61/24"
                },
                {
                    "NetworkID": "fufdtvhekbyhpfvb71zosei0k",
                    "Addr": "10.0.60.2/24"
                }
            ]
        },
        "UpdateStatus": {
            "State": "completed",
            "StartedAt": "2022-04-06T18:46:37.03796414Z",
            "CompletedAt": "2022-04-06T18:51:09.982709527Z",
            "Message": "update completed"
        }
    }
]
```

Тут также можно увидеть, какие порты слушает сервис, в каких сетях участвует, какой у него статус и много чего еще.

Удалить стэк можно следующим образом:

```
docker stack rm stage
```

Можно запускать и отдельно взятые сервисы, например:

```
docker service create --name nginx --replicas 3 nginx:alpine;
docker service ps nginx;
ID             NAME      IMAGE          NODE      DESIRED STATE   CURRENT STATE            ERROR     PORTS
awn4xed5twz3   nginx.1   nginx:alpine   stage     Running         Running 24 minutes ago             
m42te1ahbi3q   nginx.2   nginx:alpine   prod-1    Running         Running 20 minutes ago             
otkio3yfcw7o   nginx.3   nginx:alpine   dev-2     Running         Running 16 minutes ago  
```

В данном примере мы запустили сервис nginx в виде 3 экземпляров, которые swarm раскидал по 3 нодам.

Удалить сервис можно следующим образом:

```
docker service rm nginx
```

**Label**

Swarm по умолчанию развертывает сервисы на любой доступной ноде/нодах, но как правило нам необходимо развертывать их на конкретной ноде или на специфической группе. И тут нам как раз приходят на помощь labels.

Например, у нас есть stage и prod окружение. stage используется для внутренней демонстрации продукта, а prod как можно догадаться из названия, является непосредственно продакшеном.

Для каждого из окружений у нас есть compose файл: docker-compose.stage.yaml и docker-compose.prod.yaml. По умолчанию swarm будет раскидывать service произвольно по нодам. А нам бы хотелось, чтобы сервис для stage запускался только на stage виртуалке и аналогично для prod.

В начале, добавим еще одну ноду в кластер, в качестве worker:

```
# заходим по ssh на еще одну виртуальную машину и добавляем ее в кластер
docker swarm join --token SWMTKN-1-54k2k418tw2j0juwm3inq6crp4ow6xogswihcc5azg7oq5qo7e-a3rfeyfwo7d93heq0y5vhyzod

# выполняем команду на manager ноде:
docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
vj7kp5847rh5mbbqn97ghyb72 *   dev-2      Ready     Active         Leader           20.10.14
k3wjrl2o827in7k55wqfjfyxs     prod-1     Ready     Active                          20.10.14
zxo15m9wqdjd9f8pvg4gg6gwi     stage      Ready     Active                          20.10.9
```

Затем необходимо разметить наши ноды:

```
docker node update --label-add TAG=stage stage
docker node update --label-add TAG=prod prod-1
```

Используя hostname виртуалок, мы навешиваем label. Для того чтобы убедиться, что все прошло успешно, необходимо выполнить следующую команду:

```
docker node inspect stage
```

Ищем раздел Spec.Labels, где мы должны увидеть label, который добавили:

```
"Spec": {
    "Labels": {
        "TAG": "stage"
    },
    "Role": "worker",
    "Availability": "active"
},
```

После чего в наш compose файл необходимо добавить директиву placement, где прописывается условие, которое указывает, на каких нодах разворачивать данный сервис:

```
# docker-compose.stage.yaml

version: "3.9"

services:
  back:
    image: docker-registry.ru:5000/ptm:stage
    ports:
      - "4003:4003"
    environment:
      TZ: "Europe/Moscow"
    extra_hosts:
      - host.docker.internal:host-gateway
    command: make server_start
    volumes:
      - /p/ptm/config/config.yaml:/p/ptm/config/config.yaml
      - /p/ptm/stat/web:/p/ptm/stat/web
    # swarm
    deploy:
      placement:
        constraints:
          - "node.labels.TAG==stage"
```

Для docker-compose.prod.yaml будет аналогично, но с тэгом prod (однако для внешнего мира надо использовать другой порт, например 4004). После деплоя данных stacks вы убедитесь, что сервисы разворачиваются только на нодах с определенным тэгом.

**Маршрутизация**

В данный момент у нас 3 ноды: manager нода, нода для stage версии приложения и еще одна для продакшена.

```
docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
vj7kp5847rh5mbbqn97ghyb72 *   dev-2      Ready     Active         Leader           20.10.14
k3wjrl2o827in7k55wqfjfyxs     prod-1     Ready     Active                          20.10.14
zxo15m9wqdjd9f8pvg4gg6gwi     stage      Ready     Active  
```

И если мы попытаемся задеплоить наш стэк для docker-compose.prod.yaml на том же 4003 порту, что и для уже запущенного стэка docker-compose.stage.yaml, мы получим ошибку, связанную с тем, что порт уже занят.

Хммм... почему это произошло?🤔 И более того, если мы зайдем на виртуальную машину prod-1 и сделаем curl 127.0.0.1:4003, то увидим, что наш сервис доступен, хотя на этой ноде мы еще не успели ничего развернуть?


Связано это с тем, что у swarm имеется ingress сеть, которая используется для маршрутизации траффика между нодами. Если у сервиса есть публичный порт, то swarm слушает его и в случае поступления запроса на него делает следующее: определяет есть ли контейнер на этой хост машине и если нет, то находит ноду, на которой запущен контейнер для данного сервиса, и перенаправляет запрос туда.


В данном примере используется внешний балансировщик HAProxy, который балансирует запросы между тремя виртуалками, а дальше swarm перенаправляет запросы в соответствующие контейнеры.

Вот почему придется для docker-compose.prod.yaml использовать любой другой публичный порт, отличный от того, который мы указали в docker-compose.stage.yaml.

Отключить автоматическую маршрутизацию трафика можно с помощью mode: host при декларации ports:

```
# docker-compose.prod.yaml
version: "3.9"

services:
  back:
    image: docker-registry.ru:5000/ptm:stage
    ports:
		  - target: 4003
    		published: 4004
		    protocol: tcp
    		mode: host
    environment:
      TZ: "Europe/Moscow"
    extra_hosts:
      - host.docker.internal:host-gateway
    command: make server_start
    volumes:
      - /p/ptm/config/config.yaml:/p/ptm/config/config.yaml
      - /p/ptm/stat/web:/p/ptm/stat/web
    # swarm
    deploy:
      placement:
        constraints:
          - "node.labels.TAG==prod"
```

В данном случае запрос, который придет на порт 4004, swarm будет направлять только на контейнер текущей ноде и никуда больше.

Кроме того, надо упомянуть про такую настройку как mode, хотя напрямую это не относится к маршрутизации. Она может принимать следующие значения: global или replicated (по умолчанию):

```
deploy:
	mode: global

deploy:
	mode: replicated
  replicas: 4
```

Если global это означает, что данный сервис будет запущен ровно в одном экземпляре на всех возможных нодах. А replicated означает, что n-ое кол-во контейнеров для данного сервиса будет запущено на всех доступных нодах.

Кроме того, советую почитать следующую статью про то, как устроены сети в swarm.

**Zero downtime deployment**

Одна из очень полезных фишек, которая есть из коробки, это возможность организовать бесшовную смену контейнеров во время деплоя. Да, с docker-compose это тоже возможно, но надо писать обвязку на bash/ansible или держать дополнительную реплику контейнера (даже, если по нагрузке требуется всего одна) и на уровне балансировщика переключать трафик с одного контейнера на другой.

С docker swarm это все не нужно, необходимо лишь немного скорректировать конфиг сервиса.

Для начала нужно добавить директивуhealthcheck:

```
healthcheck:
  test: curl -sS http://127.0.0.1:4004/ptm/api/healthcheck || echo 1
  interval: 30s
  timeout: 3s
  retries: 12
```

Она предназначена, чтобы docker мог определить, корректно ли работает ваш контейнер.

test - результат исполнения этой команды docker использует для определения корректно ли работает контейнер.

interval - с какой частотой проверять состояние. В данном случае каждые 30 секунд.

timeout - таймаут для ожидания выполнения команды.

retries - кол-во попыток для проверки состояния нашего сервера внутри контейнера.

После добавления данной директивы в compose file мы увидим в колонке Status информацию о жизненном состоянии контейнера.

```
docker container ls
CONTAINER ID   IMAGE                               COMMAND                  CREATED        STATUS                PORTS      NAMES
b66aa4c93fda   docker-registry.ru:5000/ptm:stage   "docker-entrypoint.s…"   2 days ago     Up 2 days (healthy)   4003/tcp   prod_back.1.6x33tl8sj9t5ddlbbm2rrxbrn
```

После того как вы сделаете deploy вашему стэку, вы увидите, что контейнер сначала имеет статус starting (сервер внутри нашего контейнера запускается), а через некоторое время получит статус healthy (сервер запустился), в противном случае unhealthy. Docker в режиме swarm не просто отслеживает жизненное состояние контейнера, но и в случае перехода в состояние unhealthy попытается пересоздать контейнер.

После того, как мы научили docker следить за жизненным состоянием контейнера необходимо добавить настройки для обновления контейнера:

```
deploy:
  replicas: 1
  update_config:
    parallelism: 1
    order: start-first
    failure_action: rollback
    delay: 10s
```

replicas это кол-во контейнеров, которые необходимо запустить для данного сервиса.

Директива update_config описывает каким образом сервис должен обновляться:

parallelism - кол-во контейнеров для одновременного обновления. По умолчанию данный параметр имеет значение 1 - контейнеры будут обновляться по одному. 0 - обновить сразу все контейнеры. В большинстве случаев это число должно быть меньше, чем общее кол-во реплик вашего сервиса.

order - Порядок обновления контейнеров. По умолчанию stop-first, сначала текущий контейнер останавливается, а затем запускается новый. Для бесшовного обновления нам нужно использовать start-first это когда вначале запускается новый контейнер, а затем выключается старый.

failure_action- стратегия в случае сбоя. Вариантов несколько:  continue, rollback, или pause (по умолчанию).

delay - задержка между обновлением группы контейнеров.

Кроме того, полезна директиваrollback_config, которая описывает поведение в случае сбоя во время обновления, а также директива restart_policy, которая описывает когда и как перезапускать контейнеры в случае проблем.

```
deploy:
  replicas: 1
  update_config:
    parallelism: 1
    order: start-first
    failure_action: rollback
    delay: 10s
  rollback_config:
    parallelism: 0
    order: stop-first
  restart_policy:
    condition: any
    delay: 5s
    max_attempts: 3
    window: 120s
```

condition - есть несколько возможных вариантов none, on-failure or any.

delay - как долго ждать между попытками перезапуска.

max_attempts- максимальное кол-во попыток для перезапуска.

window - как долго ждать прежде, чем определить, что рестарт удался.

**Итоговый docker-compose.yml**

Для обновления нашего сервиса в запущенном стэке надо выполнить следующую команду:

```
# pull image 
docker pull docker-registry.ru:5000/ptm:stage;
docker service update --image docker-registry.ru:5000/ptm:stage stage;
```

Затем, если выполнить docker container ls на worker ноде, то мы увидим, что запустился новый контейнер со статусом starting, а когда он станет healthy, то swarm переключит трафик на новый контейнер, а старый остановит.

```
CONTAINER ID   IMAGE                               COMMAND                  CREATED         STATUS                           PORTS                                       NAMES
2fd259e82ade   docker-registry.ru:5000/ptm:stage   "docker-entrypoint.s…"   3 seconds ago   Up 1 second (health: starting)   4003/tcp                                    stage_back.1.bxa0a1y86mnsi8yyd0wpxhc7j
7e726dbc2f0d   docker-registry.ru:5000/ptm:stage   "docker-entrypoint.s…"   25 hours ago    Up 25 hours (healthy)            4003/tcp                                    stage_back.1.q20unddw9v9i3tdojohckv7cw
```

Благодаря директивам rollback_config и restart_policy мы описали, что будет делать swarm c контейнером в случае, если не удалось его запустить и в каком случае перезапускать контейнер, а также максимальное кол-во попыток и задержек между ними.

Вот так с помощью десятка строчек мы получили бесшовный деплой для наших контейнеров.

**Секреты**

Swarm предоставляет хранилище для приватных данных (secrets), которые необходимы контейнерам. Как правило эта функциональность используется для хранения логинов, паролей, ключей шифрования и токенов доступа от внешних систем, БД и т.д.

Создадим yaml файл c «суперсекретным» токеном:

```
# example.yaml
token: sfsjksajflsf_secret
```

Создадим секрет с именем back_config:

```
docker secret create back_config example.yaml 
nys6v16j4d8ymmif87sq6w305

# show secrets
docker secret ls
ID                          NAME          DRIVER    CREATED         UPDATED
nys6v16j4d8ymmif87sq6w305   back_config             9 seconds ago   9 seconds ago

# inspect specific secret
docker secret inspect back_config
[
    {
        "ID": "nys6v16j4d8ymmif87sq6w305",
        "Version": {
            "Index": 24683
        },
        "CreatedAt": "2022-04-13T15:19:14.474608684Z",
        "UpdatedAt": "2022-04-13T15:19:14.474608684Z",
        "Spec": {
            "Name": "back_config",
            "Labels": {}
        }
    }
]
```

Для того чтобы им воспользоваться нам надо добавить 2 секции. Во-первых, секцию secrets, где мы укажем, что берем снаружи (из swarm) секрет под именем back_config. Во-вторых, подключим сам секрет в сервисе (тоже директива secrets) по пути /p/ptm/config/config.json.

```
version: "3.9"

services:
  back:
    image: docker-registry.ru:5000/ptm:stage
    ports:
       - "4004:4003"
    environment:
      TZ: "Europe/Moscow"
    extra_hosts:
      - host.docker.internal:host-gateway
    command: make server_start
    volumes:
      - /p/ptm/stat/web:/p/ptm/stat/web
    secrets:
      - source: back_config
        target: /p/ptm/config/config.yaml
    # swarm
    deploy:
      placement:
        constraints:
          - "node.labels.TAG==prod"
      replicas: 1
      update_config:
        parallelism: 1
        order: start-first
        failure_action: rollback
        delay: 10s
      rollback_config:
        parallelism: 0
        order: stop-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
    healthcheck:
      test: curl -sS http://127.0.0.1:4004/ptm/api/healthcheck || echo 1
      interval: 30s
      timeout: 3s
      retries: 12
  
secrets:
  back_config:
    external: true
```

Ранее мы монтировали config.yaml, как volume, теперь же мы достаем его из swarm и монтируем его по указанному пути. Если мы теперь зайдем внутрь контейнера, то обнаружим, что наш секрет находится в /p/ptm/config/config.yaml.

```
# enter to container
docker exec -it $container_id sh

cat /p/ptm/config/config.yaml;
```

**Portainer**

Кроме того, хочу упомянуть про такой инструмент как portainer. Вместо того, чтобы лазить постоянно на manager node и выполнять рутинные команды в консоли, можно использовать web панель для управления docker swarm. Это довольно удобный инструмент, позволяющий посмотреть в одном месте данные о стэках, сервисах и контейнерах. Можно отредактировать существующие сервисы, а также запустить новые. Кроме того, можно посмотреть логи запущенных контейнеров и даже зайти внутрь них удаленно. Ко всему прочему portainer может выступать в роли docker registry, а также предоставляет управление секретами и конфигами.

Для того чтобы его установить необходимо в начале скачать .yml файл с описанием сервисов:

```
curl -L https://downloads.portainer.io/portainer-agent-stack.yml \
    -o portainer-agent-stack.yml
```

Затем задеплоить его в наш кластер:

```
docker stack deploy -c portainer-agent-stack.yml portainer
```

На каждом node будет установлен агент, который будет собирать данные, а на manager будет установлен сервер с web панелью.

Надо не забыть открыть порт 9443 на виртуальной машине, которая является manager:

```
firewall-cmd --add-port=9443/tcp --permanent;
firewall-cmd --reload;
```

Заходим в браузере на адрес https://ip:9443.

Соглашаемся на то, что доверяем https сертификату (portainer, по умолчанию создает самоподписанный). Создаем пользователя и наслаждаемся.


**Итог**

Docker Swarm это несложный оркестратор для контейнеров, который доступен из коробки. Поверхностно с ним можно разобраться за пару дней, а глубоко за неделю. И по моему мнению, он закрывает большинство потребностей для маленьких и средних команд. В отличие от Kuberneteus он гораздо проще в освоении и не требует выделения дополнительных ресурсов в виде людей (отдельного человека или даже отдела) и железа. Если ваша команда разработки меньше 100 человек и вы не запускаете сотни уникальных типов контейнеров, то вам скорее всего вполне хватит его возможностей.

## 9.2.	Запуск Docker в режиме Swarm](#)



## 9.3.	Управление узлами Docker



## 9.4.	Работа с сервисами в Docker Swarm](#)



## 9.5.	Работа с сетями в Docker Swarm



## 9.6.	Работа с разделами в Docker Swarm



## 9.7.	Работа со стаками в Docker Swarm



# 10.	Безопасность Docker

## 10.1.	Основы безопасности Docker

Контейнеры Docker уже довольно давно стали неотъемлемой частью инструментария разработчика, позволяя собирать, распространять и развертывать приложения стандартизированным способом.

Неудивительно, что при такой популярности наблюдается всплеск проблем безопасности, связанных с контейнеризацией. Также контейнеры представляют собой стандартизированную поверхность атаки: злоумышленники могут легко эксплуатировать ошибки в конфигурации и проникать из контейнера в хост-машину.

Кроме того, термин "контейнер" часто понимается неправильно: многие разработчики склонны ассоциировать концепцию изоляции с ложным чувством безопасности, полагая, что эта технология безопасна по своей сути.

Ключевым моментом здесь является то, что конфигурация контейнеров по умолчанию не ориентирована на безопасность. Их защищенность полностью зависит от:

- сопутствующей инфраструктуры (ОС и платформа);

- встроенных в них программных компонент;

- конфигурации в рантайме.

Безопасность контейнеров — это очень обширная тема, но хорошая новость состоит в том, что многие лучшие практики лежат на поверхности и их очень просто использовать для уменьшения поверхности атаки.

Именно поэтому мы подготовили рекомендации по сборке и запуску Docker-контейнеров.

РИСУНОК СХЕМА

Примечание: в управляемой среде, такой как Kubernetes, большинство этих параметров могут быть переопределены контекстом безопасности или другими правилами безопасности более высокого уровня. Подробнее здесь.

**Сборка образа**

Проверьте свои образы
Внимательно выбирайте базовый образ, когда выполняете docker pull image:tag

Лучше всегда использовать проверенный образ, предпочтительно из Docker Official Images, чтобы смягчить атаки на цепочки поставок (supply-chain attacks).

В качестве базового дистрибутива рекомендуется выбирать Alpine Linux, поскольку это один из самых легких доступных дистрибутивов, что обеспечивает уменьшение поверхности атаки.

Что лучше: использовать версию с фиксированным тегом или latest?

Необходимо понимать, что теги Docker работают от менее специфичных к более специфичным, например:

```
python:3.9.6-alpine3.14

python:3.9.6-alpine

python:3.9-alpine

python:alpine
```

Все эти теги относятся к одному и тому же образу (на момент написания статьи).

Указывая более конкретную версию и фиксируя ее, вы защищаете себя от любых будущих критических изменений (breaking change). С другой стороны, использование последней версии гарантирует исправление большего количества уязвимостей. Это компромисс. Хорошая практика — привязка к стабильной версии.

Учитывая это, мы бы выбрали python:3.9-alpine.

Примечание: то же самое относится к пакетам, устанавливаемым в процессе сборки вашего образа.

Всегда используйте непривилегированного пользователя
По умолчанию процесс внутри контейнера запускается от root (id = 0).

Для реализации принципа наименьших привилегий вы должны настроить пользователя. Сделать это можно двумя способами:

Указать с помощью параметра -u произвольный ID пользователя, которого нет в запущенном контейнере:

```
docker run -u 4000 <image>
```

Примечание: если впоследствии вам понадобится смонтировать файловую систему, то для получения доступа к файлам вам потребуется сопоставить используемый ID пользователя с пользователем хоста.

Или заранее создать пользователя в Dockerfile:

```
FROM <базовый образ>
RUN addgroup -S appgroup </span>
 && adduser -S appuser -G appgroup 

USER appuser
... <продолжение Dockerfile> ...
```

Примечание: для управления пользователями и группами используйте утилиты, входящие в ваш базовый образ.

**Используйте отдельный User ID namespace**

По умолчанию демон Docker использует User namespace хоста. Следовательно, любое успешное повышение привилегий внутри контейнера будет также означать получение root-доступа как к хосту, так и к другим контейнерам.

Чтобы снизить этот риск, необходимо настроить хост и демон Docker на использование отдельного пространства имен с помощью параметра --userns-remap. Подробнее здесь.

Внимательно обращайтесь с переменными окружения
Никогда не указывайте конфиденциальную информацию в открытом виде в директиве ENV. Это место небезопасно для хранения информации, которую вы не хотите видеть в последнем слое образа. Например, если вы думаете, что использование unset следующим образом обеспечит вам безопасность.

```
ENV $VAR
RUN unset $VAR
```

Вы ошибаетесь! $VAR все еще будет присутствовать в контейнере и может быть получен в любое время!

Чтобы предотвратить доступ к переменной в рантайме, используйте одну команду RUN для установки и очистки переменной в одном слое (не забывайте, что переменную все еще можно извлечь из образа).

```
RUN export ADMIN_USER="admin" \
    && ... \
    && unset ADMIN_USER
```

Лучше используйте директиву ARG (значения ARG недоступны после создания образа).

К сожалению, секреты слишком часто жёстко закодированы в слоях Docker-образов, поэтому для их поиска мы разработали инструмент сканирования, использующий движок секретов GitGuardian:

ggshield scan docker <image>

Не предоставляйте доступ к сокету демона Docker
Если вы не уверены абсолютно в том, что делаете, никогда не открывайте UNIX-сокет, который слушает Docker: /var/run/docker.sock

Это основная точка входа для Docker API. Предоставление доступа к нему равносильно предоставлению неограниченного root-доступа к вашему хосту.

Никогда не открывайте его другим контейнерам:

```
-v /var/run/docker.sock://var/run/docker.sock
```

**Привилегии, возможности (capabilities) и общие ресурсы**

Во-первых, ваш контейнер никогда не должен запускаться как привилегированный, иначе у него будут все права root на хост-машине.

Лучше явно запретите добавление новых привилегий после создания контейнера с помощью опции 

```
--security-opt=no-new-privileges.
```

Во-вторых, capabilities — это механизм Linux, используемый Docker для превращения двоичной дихотомии "root / не root" в детализированную систему контроля доступа: ваши контейнеры запускаются с определенным набором capabilities по умолчанию, которые, скорее всего, вам все не нужны.

Рекомендуется не использовать capabilities по умолчанию, а удалить их все и явно указать только нужные: см. список capabilities по умолчанию.

Например, веб-серверу, вероятно, потребуется только NET_BIND_SERVICE для привязки к порту ниже 1024 (например, к порту 80).

В-третьих, не расшаривайте чувствительные части хостовой файловой системы:

корень (/);

устройства (/dev);

процессы (/proc);

виртуальные точки монтирования (/sys).

Если вам нужен доступ к устройствам хоста, будьте внимательны и выборочно включите доступ с помощью флагов [r|w|m] (чтение, запись и используйте mknod).

**Используйте контрольные группы для ограничения доступа к ресурсам**

Контрольные группы (Control Groups, cgroup) — это механизм, используемый для управления доступом контейнеров к процессору, памяти и операциям ввода-вывода.

По умолчанию для контейнера выделяется отдельная cgroup, но если вы укажете параметр --cgroup-parent, то подвергните ресурсы хоста риску DoS-атаки, поскольку появляются разделяемые ресурсы между хостом и контейнером .

По той же причине рекомендуется ограничивать использование памяти и процессора с помощью параметров:

```
--memory=”400m”
--memory-swap=”1g”

--cpus=0.5
--restart=on-failure:5
--ulimit nofile=5
--ulimit nproc=5
```

Подробнее об ограничении ресурсов см. здесь

**Файловая система**

Запретите изменение корневой файловой системы
Контейнеры должны быть эфемерными и без состояния. Поэтому часто файловую систему можно монтировать только для чтения.

```
docker run --read-only <image>
```

Используйте временную файловую систему 
Если вам нужно только временное хранилище, то используйте соответствующий параметр.

```
docker run --read-only --tmpfs /tmp:rw ,noexec,nosuid <image>
```

**Долговременное хранение данных**

Для долговременного хранения данных у вас есть два варианта:

- монтирование каталогов хоста (bind mount) с ограничением доступного пространства (--mount type=bind, o=size)

- использование томов (volume) (--mount type=volume).

В обоих случаях, если контейнер не должен изменять данные, монтируйте его только для чтения.

```
docker run -v <volume-name>:/path/in/container:ro <image>
```

или

```
docker run --mount source=<volume-name>,destination=/path/in/container,readonly <image>
```

**Сеть**

Не используйте bridge-интерфейс по умолчанию docker0
docker0 — это сетевой мост, который создается автоматически и используется для изоляции сети хоста от сети контейнера.

По умолчанию контейнеры подключаются к сети docker0 и могут взаимодействовать друг с другом.

Лучше всегда отключать это поведение по умолчанию через параметр --bridge=none, и создавать отдельные сети для каждого соединения с помощью команды:

```
docker network create <network_name>

docker run --network=<network_name>
```

**Простой пример сети Docker**

Например, для веб-сервера, взаимодействующего с базой данных (запущенной в другом контейнере), лучше всего создать bridge-сеть WEB для маршрутизации входящего трафика с сетевого интерфейса хоста и еще один bridge — DB для связи между контейнерами веб-сервера и базы данных.

**Не используйте network namespace хоста**

То же самое, изолируйте сетевой интерфейс хоста: не используйте host-сеть (--net=host).

**Логирование**

По умолчанию уровень логирования — INFO, но вы можете указать другой с помощью параметра:

```
--log-level="debug"|"info"|"warn"|"error"|"fatal"
```

Менее известна возможность экспорта логов Docker: можно перенаправить потоки STDERR и STDOUT на внешний сервис логирования, используя параметр 

```
--log-driver=<logging_driver>
```

Также можно настроить двойное логирование, чтобы при использовании внешнего сервиса сохранить доступ Docker к логам. Если ваше приложение пишет логи в специальные файлы (обычно в /var/log), то их тоже можно перенаправить (см. официальную документацию).

**Использование достоверных образов**

Начнем, пожалуй, с банальной для репозиториев Docker проблемы — достоверности образа. Чтобы избежать проблем с поддельными образами, используйте приватные (private) или строго доверенные репозитории (trusted repositories) вроде Docker Hub. В отличие от других подобных репозиториев, образы, которые там лежат, всегда сканирует и просматривает специальный сканер безопасности Docker’s Security Scanning Service.

**Верифицируем образы через сервис Docker Content Trust**

Еще один полезный и доступный всем инструмент, который стоит использовать, — Docker Content Trust. Это новая функция, доступная с версии Docker Engine 1.8. Она позволяет верифицировать владельца образа. Таким образом этот сервис защищает вас от фейков и подделок, атак повторного воспроизведения и компрометации ключей.

**CVE-уязвимости в контейнерах**

Контейнеры Docker — это, по сути, изолированные черные ящики, которые крутятся на родительском хосте. Однако все может отлично работать, но при этом внутри оказывается уязвимое ПО. Причем в апстриме уязвимости могут быть давно пропатчены, но не в вашем локальном образе! И если не принять меры, такие проблемы могут долго оставаться незамеченными.

Многие реестры образов Docker предлагают услугу сканирования этих самых образов. Например, сервис CoreOS Quay использует сканер безопасности образов Docker с открытым исходным кодом под названием Clair. Clair — это приложение, написанное на Go, которое реализует набор HTTP API для выгрузки, загрузки и анализа образов. Данные об уязвимостях загружаются из разных источников, таких как Debian Security Tracker или Red Hat Security Data. В таком случае движок Clair работает по принципу статического анализатора без запуска контейнера на выполнение.

Ну и если вы начинающий админ или руководство адски зажимает денег, где-то на просторах Сети, говорят, есть аналогичные сервисы с теми же возможностями, но при этом совершенно бесплатные! Правда, я таких пока не встречал. Поэтому, если уж совсем нет выбора, можно использовать старый добрый OpenVAS либо аналогичный сканер безопасности, который вы будете запускать ручками по расписанию внутри своей корпоративной сетки.

**Сканирование на уязвимости и секреты**

И последнее, но не менее важное: надеюсь, теперь стало понятно, что ваши контейнеры настолько безопасны, насколько безопасно выполняемое в них программное обеспечение. Для проверки на уязвимости есть много разных инструментов как платных, так и бесплатных.

Проверка хоста и контейнера с помощью Docker Bench Security

Очень полезный инструмент, которым я сам не раз пользовался, — это Docker Bench Security (см. также документацию к нему). По сути, это большая подборка рекомендаций, советов и практик по развертыванию контейнеров в продакшене. Инструмент основывается на рекомендациях из документа The CIS Docker 1.13 Benchmark (PDF) и применяется в следующих областях:

конфигурация хоста (ядро, процессы, разрешения);
конфигурация демона Docker (сеть, выделение RAM, CPU);
файлы конфигурации демона Docker;
образы контейнеров и build-файлы;
Runtime контейнера;
опции «по умолчанию» Docker security.
Чтобы установить этот скрипт проверки, клонируйте репозиторий следующей командой в терминале:

```
$ git clone git@github.com:docker/docker-bench-security.git
```

После этого запускайте скрипт:

```
$ cd docker-bench-secutity
```

и используйте такую вот длинную команду:

```
$ docker run -it --net host --pid host --cap-add audit_control \
  -e DOCKER_CONTENT_TRUST=$DOCKER_CONTENT_TRUST \
  -v /var/lib:/var/lib \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -v /etc:/etc --label docker_bench_security \
  docker/docker-bench-security
```

Выполнив перечисленные шаги, вы соберете все контейнеры и запустите скрипт, который проверит безопасность хост-машины (kernel OS) и ее контейнеров. И это займет каких-то несколько минут!

Сканеры уязвимостей:

Бесплатные:

Clair

Trivy

Docker Bench for Security

Коммерческие:

Snyk (open source, доступна бесплатная версия)

Anchore (open source, доступна бесплатная версия)

JFrog XRay

Qualys

Сканирование секретов:

ggshield (open source, доступна бесплатная версия)

SecretScanner (бесплатная)





# 11.	Вопросы производительности






---
# Пример docker-compose 3 приложения

```
version: "3.5"
sevices:
  php-app:
   images: php-apache  если тут прописать " build: . " то вместо скачивания с регисти будет собираться из Dockerfile 
   container_name: app
   ports:
     - '80:80'
   restart: unless-stopped
   depends_on:
     - app-db
     - app-redis
   networks:
     - internet
     - appnet
  
  app-db:
   images: postgres
   container_name: app-postgres
   restart: unless-stopped
   environment:
    - 'POSTGRES_PASSWORD=mypass'
   networks:
    - appnet
  
  app-redis:
   images: redis
   container_name: app-redis
   restart: unless-stopped
   networks:
     - appnet
networks:
 internet:
     name: internet
     driver: bridge
 appnet:
    name: appnet
    driver: bridge     
```

# Пример docker 3 приложения

```
docker run 
--name app 
-p 80:80 
--net appnet 
php:apache

docker run
--name app-postgres
-e 'POSTGRES_PASSWORD=mypass'
--net appnet
postgres

docker run
--name app-redis
--net appnet
redis
```
# Пример docker в 1 строчку
```
docker run 
```
добавляем проброс порта из контейнера
```
docker run -p 80:80 -p 443:443 -d nginx:stable
```
добавляем имя контейнера
```
docker run --name myapp-nginx -p 80:80 -p 443:443 -d nginx:stable
```
добавляем volumes
```
docker run --name myapp-nginx -v /opt/web/html:/var/www/html -v /opt/web/pics:/var/www/pictures -p 80:80 -p 443:443 -d nginx:stable
```
добавляем environment
```
docker run --name myapp-nginx -v /opt/web/html:/var/www/html -v /opt/web/pics:/var/www/pictures -e NGINX_HOSTS=ekdeus.me -e NGINX_PORT=80 -p 80:80 -p 443:443 -d nginx:stable
```
добавляем сеть
```
docker run --name myapp-nginx -v /opt/web/html:/var/www/html -v /opt/web/pics:/var/www/pictures -e NGINX_HOSTS=ekdeus.me -e NGINX_PORT=80 -p 80:80 -p 443:443 --net webnet -d nginx:stable
```

теперь тоже самое запишем

в удобочитаемом виде

docker:
```
docker run 
--name myapp-nginx 
-v /opt/web/html:/var/www/html 
-v /opt/web/pics:/var/www/pictures 
-e NGINX_HOSTS=ekdeus.me 
-e NGINX_PORT=80 
-d 
-p 80:80 
-p 443:443 
--net webnet 
nginx:stable
```
и теперь тоже самое в docker-compose

```
version: "3.5"
services:
  images: nginx:stable
  container_name: myapp-nginx 
  volumes:
    - /opt/web/html:/var/www/html 
    - /opt/web/pics:/var/www/pictures
  environment: 
    - NGINX_HOSTS=ekdeus.me 
    - NGINX_PORT=80
  ports:
    - 80:80 
    - 443:443
  restart: unless-stopped # always/no/on-failure
networks:
  default:
    driver: bridge
    name: webnet
```


ну и для запуска можно использовать `docker-compose up -d`  или `docker compose up -d` 
